---
title: Reproducing GPT-2
description: Train GPT-2 (124M) on OpenWebText to reproduce OpenAI's results
---

# Reproducing GPT-2 (124M)

This guide shows how to reproduce OpenAI's GPT-2 (124M) results using nanoGPT. On an 8×A100 40GB node, training takes about 4 days and reaches a validation loss of ~2.85.

## Overview

We'll train a 124M parameter GPT-2 model on OpenWebText, an open reproduction of OpenAI's private WebText dataset. The default configuration in `config/train_gpt2.py` is tuned to reproduce GPT-2's performance.

<Info>
  **Expected results**: Validation loss of ~2.85 after ~4 days on 8×A100 40GB GPUs. This matches OpenAI's GPT-2 performance when accounting for the domain gap between WebText and OpenWebText.
</Info>

## Prerequisites

### Hardware Requirements

<Tabs>
  <Tab title="Recommended">
    **8×A100 40GB GPUs** (single node)
    - Training time: ~4 days
    - Expected loss: ~2.85
    - Cost: ~$25/hour on cloud providers
    - Total cost: ~$2,400
  </Tab>
  
  <Tab title="Minimum">
    **1×A100 40GB GPU** (or equivalent)
    - Training time: ~32 days
    - Must adjust `gradient_accumulation_steps` to maintain batch size
    - Budget option for learning/experimentation
  </Tab>
  
  <Tab title="Budget Option">
    **4×RTX 4090 or 8×RTX 3090**
    - Training time: ~5-7 days
    - May need to adjust batch size for 24GB VRAM
    - Good balance of cost and time
  </Tab>
</Tabs>

### Software Requirements

```bash
pip install torch numpy transformers datasets tiktoken wandb tqdm
```

- PyTorch 2.0+ (for `torch.compile`)
- CUDA 11.8+ (for optimal performance)
- ~30GB disk space for dataset

## Step 1: Prepare OpenWebText

Download and tokenize the OpenWebText dataset:

```bash
python data/openwebtext/prepare.py
```

<Steps>
  <Step title="Download Dataset">
    Downloads the OpenWebText dataset from HuggingFace (~12GB compressed)
  </Step>
  
  <Step title="Tokenize with GPT-2 BPE">
    Uses OpenAI's tiktoken library to apply GPT-2's BPE tokenization
  </Step>
  
  <Step title="Save Binary Format">
    Creates memory-mapped binary files for efficient training:
    - `train.bin` - ~17GB, 9B tokens
    - `val.bin` - ~8.5MB, 4M tokens
  </Step>
</Steps>

**Dataset statistics:**
- Training tokens: 9,035,582,198
- Validation tokens: 4,434,897
- Source documents: 8,013,769
- Vocabulary size: 50,257 (GPT-2 BPE)

<Note>
  Preprocessing takes 30-60 minutes and requires ~30GB disk space. The process is single-threaded but only needs to run once.
</Note>

## Step 2: Review Configuration

Examine `config/train_gpt2.py`:

```python
# Weights & Biases logging
wandb_log = True
wandb_project = 'owt'
wandb_run_name = 'gpt2-124M'

# Batch size configuration
# Total: 12 * 1024 * 40 * 8 = 491,520 tokens/iter (~0.5M)
batch_size = 12                    # micro-batch per GPU
block_size = 1024                  # context length
gradient_accumulation_steps = 5 * 8  # 40 steps

# Training duration (300B tokens total)
max_iters = 600000
lr_decay_iters = 600000

# Evaluation settings
eval_interval = 1000
eval_iters = 200
log_interval = 10

# Regularization
weight_decay = 1e-1
```

### Understanding Batch Size

The effective batch size is carefully tuned:

```
Effective batch = batch_size × block_size × gradient_accumulation_steps × num_gpus
                = 12 × 1024 × 40 × 8
                = 491,520 tokens/iteration
                ≈ 0.5M tokens/iteration
```

This matches GPT-2's training configuration.

<Warning>
  If using fewer GPUs, adjust `gradient_accumulation_steps` proportionally to maintain the same effective batch size.
</Warning>

### Training Budget

```
Total tokens = tokens_per_iter × max_iters
             = 491,520 × 600,000
             = 294,912,000,000 tokens
             ≈ 300B tokens
```

This is less than GPT-2's 300B tokens due to dataset size constraints.

## Step 3: Launch Training

<Tabs>
  <Tab title="Single Node (8 GPUs)">
    **Launch with torchrun:**
    
    ```bash
    torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
    ```
    
    **Explanation:**
    - `--standalone`: Single-node training
    - `--nproc_per_node=8`: Use all 8 GPUs
    - Uses NCCL backend for GPU communication
    
    **Expected output:**
    ```
    tokens per iteration will be: 491,520
    Initializing a new model from scratch
    found vocab_size = 50257 (inside data/openwebtext/meta.pkl)
    compiling the model... (takes a ~minute)
    step 0: train loss 10.9531, val loss 10.9532
    iter 1000: loss 6.1234, time 142.34ms, mfu 45.23%
    ```
  </Tab>
  
  <Tab title="Multi-Node (2+ Nodes)">
    **Setup for distributed training:**
    
    **On master node (e.g., IP 123.456.123.456):**
    ```bash
    torchrun \
      --nproc_per_node=8 \
      --nnodes=2 \
      --node_rank=0 \
      --master_addr=123.456.123.456 \
      --master_port=1234 \
      train.py config/train_gpt2.py
    ```
    
    **On worker node:**
    ```bash
    torchrun \
      --nproc_per_node=8 \
      --nnodes=2 \
      --node_rank=1 \
      --master_addr=123.456.123.456 \
      --master_port=1234 \
      train.py config/train_gpt2.py
    ```
    
    <Warning>
      **Without Infiniband**: Prepend `NCCL_IB_DISABLE=1` to avoid using Infiniband. Training will work but may be slower:
      
      ```bash
      NCCL_IB_DISABLE=1 torchrun ...
      ```
    </Warning>
  </Tab>
  
  <Tab title="Single GPU">
    **For experimentation or debugging:**
    
    ```bash
    python train.py config/train_gpt2.py \
      --gradient_accumulation_steps=320
    ```
    
    **Adjustments:**
    - Set `gradient_accumulation_steps = 320` (was 40 for 8 GPUs)
    - Training time: ~32 days instead of 4 days
    - Same effective batch size maintained
    
    <Note>
      Single GPU training is useful for:
      - Testing your setup
      - Debugging modifications
      - Running shorter experiments with `--max_iters=10000`
    </Note>
  </Tab>
</Tabs>

## Step 4: Monitor Training

### Console Output

```
step 1000: train loss 6.1234, val loss 6.2456
iter 1001: loss 6.1567, time 142.34ms, mfu 45.67%
step 2000: train loss 5.2341, val loss 5.3456
saving checkpoint to out
...
step 600000: train loss 2.8123, val loss 2.8456
```

**Key metrics:**
- **Train/val loss**: Should decrease from ~11 to ~2.85
- **Time per iter**: ~140-160ms on A100 with compile
- **MFU**: Model FLOPs Utilization - expect 40-50% on A100

### Weights & Biases Dashboard

If `wandb_log=True`, monitor at https://wandb.ai:

- Training/validation loss curves
- Learning rate schedule
- Model FLOPs Utilization
- GPU memory usage
- Token throughput

### Checkpoints

Checkpoints saved to `out/ckpt.pt` every 1000 iterations when validation improves:

```bash
ls -lh out/
# ckpt.pt - ~500MB checkpoint file
```

## Step 5: Evaluate Results

### Compare with OpenAI Baselines

Load OpenAI's official GPT-2 and evaluate on OpenWebText:

```bash
python train.py config/eval_gpt2.py
```

**Expected results:**

| Model | Params | Train Loss | Val Loss |
|-------|--------|------------|----------|
| gpt2 (OpenAI) | 124M | 3.11 | 3.12 |
| gpt2 (nanoGPT) | 124M | 2.81 | 2.85 |

<Info>
  **Why is nanoGPT's loss lower?**
  
  OpenAI's GPT-2 was trained on private WebText. When evaluated on OpenWebText, there's a domain gap causing higher loss (~3.11).
  
  Finetuning OpenAI's GPT-2 on OpenWebText brings it down to ~2.85, matching nanoGPT's from-scratch training.
</Info>

### Sample from Your Model

```bash
python sample.py --out_dir=out --num_samples=5 --max_new_tokens=200
```

**Example output:**

```
The researchers developed a new method for training neural networks
that significantly improves performance on language understanding tasks.
The approach, detailed in a paper published today, uses a novel
architecture that processes text more efficiently than previous models.
```

## Adjusting for Different Hardware

### Fewer GPUs

Maintain effective batch size by adjusting gradient accumulation:

```python
# For 4 GPUs (instead of 8)
gradient_accumulation_steps = 80  # was 40 for 8 GPUs

# For 2 GPUs
gradient_accumulation_steps = 160

# For 1 GPU
gradient_accumulation_steps = 320
```

**Or adjust via command line:**

```bash
torchrun --standalone --nproc_per_node=4 train.py \
  config/train_gpt2.py \
  --gradient_accumulation_steps=80
```

### Less VRAM (24GB GPUs)

Reduce micro-batch size:

```bash
python train.py config/train_gpt2.py \
  --batch_size=8 \
  --gradient_accumulation_steps=60
```

Effective batch size remains ~491K tokens.

### No bfloat16 Support

Force float16:

```bash
python train.py config/train_gpt2.py --dtype=float16
```

Gradient scaling is applied automatically.

## Training Larger Models

### GPT-2 Medium (350M)

```bash
python train.py config/train_gpt2.py \
  --n_layer=24 \
  --n_head=16 \
  --n_embd=1024 \
  --batch_size=8
```

Expected validation loss: ~2.84

### GPT-2 Large (774M)

```bash
python train.py config/train_gpt2.py \
  --n_layer=36 \
  --n_head=20 \
  --n_embd=1280 \
  --batch_size=4
```

Expected validation loss: ~2.67

<Warning>
  Larger models require:
  - More GPU memory (reduce `batch_size`)
  - Longer training time
  - More gradient accumulation steps
</Warning>

## Optimization Tips

### Enable Flash Attention

Flash Attention (automatic in PyTorch 2.0+) provides:
- 2-3× speedup for attention computation
- Reduced memory usage
- Identical numerical results

Check if enabled:

```python
# In model.py:64
self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
```

### Benchmark Your Setup

```bash
python bench.py
```

This runs the training loop without data loading/checkpointing overhead to measure pure training throughput.

### Check GPU Utilization

```bash
watch -n 1 nvidia-smi
```

Look for:
- GPU utilization: ~95-100%
- GPU memory: 35-38GB / 40GB
- Power usage: ~300-400W per A100

## Troubleshooting

### Training Loss Not Decreasing

1. Verify dataset prepared correctly:
   ```bash
   ls -lh data/openwebtext/
   # Should see train.bin (~17GB) and val.bin (~8.5MB)
   ```

2. Check learning rate schedule:
   ```python
   # Should use warmup + cosine decay
   decay_lr = True
   warmup_iters = 2000
   ```

### Out of Memory

Reduce memory usage:

```bash
python train.py config/train_gpt2.py \
  --batch_size=8 \
  --gradient_accumulation_steps=60
```

### Slow Training (Low MFU)

Possible causes:
- torch.compile disabled: Enable with `--compile=True`
- Old PyTorch version: Upgrade to 2.0+
- CPU bottleneck: Check data loading isn't blocking
- Slow interconnect: Use Infiniband or disable with `NCCL_IB_DISABLE=1`

### NaN Loss

Usually indicates:
- Learning rate too high (try `--learning_rate=3e-4`)
- Gradient clipping disabled (ensure `--grad_clip=1.0`)
- Mixed precision issues (try `--dtype=float32` for debugging)

## Checkpointing and Resuming

### Resume from Checkpoint

If training is interrupted:

```bash
python train.py config/train_gpt2.py --init_from=resume
```

This loads:
- Model weights
- Optimizer state
- Iteration counter
- Best validation loss

<Note>
  Checkpoints are automatically saved to `out/ckpt.pt` when validation loss improves or every `eval_interval` iterations if `always_save_checkpoint=True`.
</Note>

### Save More Frequent Checkpoints

```bash
python train.py config/train_gpt2.py \
  --always_save_checkpoint=True \
  --eval_interval=500
```

## Next Steps

<CardGroup cols={2}>
  <Card title="Finetuning" icon="sliders" href="/training/finetuning">
    Finetune your trained model on custom datasets
  </Card>
  
  <Card title="Distributed Training" icon="server" href="/training/distributed">
    Learn more about multi-node training optimization
  </Card>
</CardGroup>

## Key Takeaways

- GPT-2 (124M) reproduction takes ~4 days on 8×A100 40GB
- Validation loss of ~2.85 matches OpenAI's performance
- Effective batch size of ~0.5M tokens is crucial
- PyTorch 2.0 compile provides ~40% speedup
- Domain gap between WebText and OpenWebText explains baseline difference
- Checkpointing enables resuming long training runs