---
title: Finetuning Pretrained Models
description: Adapt pretrained GPT-2 models to your custom datasets
---

# Finetuning Pretrained Models

Finetuning allows you to adapt OpenAI's pretrained GPT-2 models to your specific domain or task. This is much faster than training from scratch and often produces better results with less data.

## Why Finetune?

<CardGroup cols={2}>
  <Card title="Much Faster" icon="bolt">
    Minutes to hours instead of days or weeks
  </Card>
  
  <Card title="Less Data Required" icon="database">
    Works well with small domain-specific datasets
  </Card>
  
  <Card title="Better Starting Point" icon="rocket">
    Leverages GPT-2's pretrained knowledge
  </Card>
  
  <Card title="Lower Cost" icon="dollar-sign">
    Requires fewer GPU-hours than training from scratch
  </Card>
</CardGroup>

## Quick Start: Finetune on Shakespeare

Let's finetune GPT-2 XL (1.5B) on Shakespeare using the provided config:

<Steps>
  <Step title="Prepare Dataset">
    Download and tokenize Shakespeare with GPT-2 BPE:
    
    ```bash
    python data/shakespeare/prepare.py
    ```
    
    <Note>
      This uses BPE tokenization (not character-level), matching GPT-2's vocabulary.
    </Note>
  </Step>
  
  <Step title="Run Finetuning">
    Execute the finetuning config:
    
    ```bash
    python train.py config/finetune_shakespeare.py
    ```
    
    On a single A100 GPU, this takes just a few minutes.
  </Step>
  
  <Step title="Generate Samples">
    Sample from your finetuned model:
    
    ```bash
    python sample.py --out_dir=out-shakespeare
    ```
  </Step>
</Steps>

## Finetuning Configuration

Let's examine `config/finetune_shakespeare.py`:

```python
import time

# Output directory
out_dir = 'out-shakespeare'

# Frequent evaluation (small dataset)
eval_interval = 5
eval_iters = 40

# Optional W&B logging
wandb_log = False
wandb_project = 'shakespeare'
wandb_run_name = 'ft-' + str(time.time())

# Dataset and initialization
dataset = 'shakespeare'
init_from = 'gpt2-xl'  # Start from largest GPT-2 (1.5B params)

# Only save when val improves
always_save_checkpoint = False

# Batch configuration
# 1 batch * 32 grad_accum * 1024 tokens = 32,768 tokens/iter
# Shakespeare has 301,966 tokens, so 1 epoch ≈ 9.2 iters
batch_size = 1
gradient_accumulation_steps = 32
max_iters = 20

# Finetune at constant LR
learning_rate = 3e-5
decay_lr = False
```

### Key Differences from Pretraining

| Aspect | Pretraining | Finetuning |
|--------|-------------|------------|
| `init_from` | `'scratch'` | `'gpt2-xl'` |
| Learning rate | 6e-4 | 3e-5 (20× lower) |
| LR schedule | Warmup + decay | Constant |
| Max iters | 600,000 | 20 (tiny dataset) |
| Dropout | 0.0 | 0.1+ (optional) |
| Eval frequency | 2000 | 5 (frequent) |

<Info>
  **Low learning rate is crucial**: Pretrained weights are already good. Too high a learning rate will destroy the learned features.
</Info>

## Available Pretrained Models

nanoGPT can load any of OpenAI's GPT-2 models:

<Tabs>
  <Tab title="gpt2 (124M)">
    ```python
    init_from = 'gpt2'
    ```
    
    **Specifications:**
    - Parameters: 124M
    - Layers: 12
    - Heads: 12
    - Embedding dim: 768
    - Memory: ~2GB
    
    **Best for:**
    - Quick experiments
    - Limited GPU memory
    - Fast iteration
  </Tab>
  
  <Tab title="gpt2-medium (350M)">
    ```python
    init_from = 'gpt2-medium'
    ```
    
    **Specifications:**
    - Parameters: 350M
    - Layers: 24
    - Heads: 16
    - Embedding dim: 1024
    - Memory: ~4GB
    
    **Best for:**
    - Balance of speed and quality
    - Mid-range GPUs (16GB+)
  </Tab>
  
  <Tab title="gpt2-large (774M)">
    ```python
    init_from = 'gpt2-large'
    ```
    
    **Specifications:**
    - Parameters: 774M
    - Layers: 36
    - Heads: 20
    - Embedding dim: 1280
    - Memory: ~8GB
    
    **Best for:**
    - High-quality results
    - GPUs with 24GB+ memory
  </Tab>
  
  <Tab title="gpt2-xl (1.5B)">
    ```python
    init_from = 'gpt2-xl'
    ```
    
    **Specifications:**
    - Parameters: 1.5B
    - Layers: 48
    - Heads: 25
    - Embedding dim: 1600
    - Memory: ~12GB
    
    **Best for:**
    - Best possible results
    - A100 or equivalent GPUs
  </Tab>
</Tabs>

## Finetuning Your Own Dataset

### Step 1: Prepare Your Data

Create a dataset preparation script similar to `data/shakespeare/prepare.py`:

```python
import os
import pickle
import requests
import numpy as np
import tiktoken

# Download or load your text data
with open('my_data.txt', 'r', encoding='utf-8') as f:
    data = f.read()

# Split into train/val
n = len(data)
train_data = data[:int(n*0.9)]
val_data = data[int(n*0.9):]

# Encode with GPT-2 BPE
enc = tiktoken.get_encoding("gpt2")
train_ids = enc.encode_ordinary(train_data)
val_ids = enc.encode_ordinary(val_data)

print(f"train has {len(train_ids):,} tokens")
print(f"val has {len(val_ids):,} tokens")

# Export to binary files
train_ids = np.array(train_ids, dtype=np.uint16)
val_ids = np.array(val_ids, dtype=np.uint16)
train_ids.tofile(os.path.join('data/my_dataset', 'train.bin'))
val_ids.tofile(os.path.join('data/my_dataset', 'val.bin'))

# Save vocab size
meta = {
    'vocab_size': enc.n_vocab,
}
with open(os.path.join('data/my_dataset', 'meta.pkl'), 'wb') as f:
    pickle.dump(meta, f)
```

<Warning>
  Always use GPT-2's BPE tokenizer (tiktoken) for finetuning. Don't use character-level or other tokenizers.
</Warning>

### Step 2: Create Config File

Create `config/finetune_my_dataset.py`:

```python
import time

out_dir = 'out-my-dataset'
eval_interval = 10
eval_iters = 50
wandb_log = False

dataset = 'my_dataset'
init_from = 'gpt2-xl'  # or gpt2, gpt2-medium, gpt2-large

always_save_checkpoint = False

# Adjust based on your dataset size
batch_size = 1
gradient_accumulation_steps = 32
max_iters = 100  # Increase for larger datasets

# Low learning rate for finetuning
learning_rate = 3e-5
decay_lr = False

# Optional: Add dropout for regularization
# dropout = 0.1
```

### Step 3: Determine Training Duration

Calculate epochs based on dataset size:

```python
tokens_per_iter = batch_size * gradient_accumulation_steps * block_size
epochs = (tokens_per_iter * max_iters) / total_tokens_in_dataset
```

**Guidelines:**
- **Small datasets (&lt;1M tokens)**: 20-50 iterations (5-20 epochs)
- **Medium datasets (1M-10M tokens)**: 100-500 iterations (1-5 epochs)
- **Large datasets (&gt;10M tokens)**: 1000+ iterations (&lt;1 epoch)

<Note>
  Finetuning typically requires fewer epochs than training from scratch. Start with fewer iterations and increase if needed.
</Note>

### Step 4: Tune Hyperparameters

Key hyperparameters to adjust:

#### Learning Rate

```python
# Conservative (safe starting point)
learning_rate = 1e-5

# Moderate (usually works well)
learning_rate = 3e-5

# Aggressive (faster but riskier)
learning_rate = 1e-4
```

<Tabs>
  <Tab title="Too High">
    **Symptoms:**
    - Training loss increases
    - Validation loss increases
    - Model generates nonsense
    
    **Solution:** Reduce by 3-10×
  </Tab>
  
  <Tab title="Too Low">
    **Symptoms:**
    - Loss decreases very slowly
    - Takes many iterations to converge
    - May not adapt to new domain
    
    **Solution:** Increase by 2-3×
  </Tab>
  
  <Tab title="Just Right">
    **Signs:**
    - Steady loss decrease
    - Val loss tracks train loss
    - Generated samples improve
    - Converges in reasonable time
  </Tab>
</Tabs>

#### Dropout

Add regularization for small datasets:

```python
# No dropout (default)
dropout = 0.0

# Light regularization
dropout = 0.1

# Heavy regularization (very small datasets)
dropout = 0.2
```

Dropout prevents overfitting but slows down adaptation.

#### Batch Size

```python
# Small datasets: smaller effective batch
batch_size = 1
gradient_accumulation_steps = 8

# Medium datasets: moderate batch
batch_size = 1
gradient_accumulation_steps = 32

# Large datasets: larger batch
batch_size = 4
gradient_accumulation_steps = 32
```

## Advanced Techniques

### Learning Rate Scheduling

Use decay for longer finetuning:

```python
# Enable decay
decay_lr = True
warmup_iters = 10      # Short warmup
lr_decay_iters = 500   # Decay over training
min_lr = 3e-6          # 10× lower than starting LR
```

### Layer-Specific Learning Rates

Manually modify `train.py` to use different rates for different layers:

```python
# In train.py, modify optimizer setup:
param_groups = [
    {'params': model.transformer.wte.parameters(), 'lr': learning_rate},
    {'params': model.transformer.h.parameters(), 'lr': learning_rate * 0.1},
    {'params': model.lm_head.parameters(), 'lr': learning_rate},
]
optimizer = torch.optim.AdamW(param_groups, ...)
```

Lower learning rates for middle layers preserve pretrained features.

### Gradual Unfreezing

Freeze early layers initially:

```python
# In train.py, after model initialization:
# Freeze first 24 layers (adjust based on model size)
for i in range(24):
    for param in model.transformer.h[i].parameters():
        param.requires_grad = False
```

Then unfreeze progressively as training continues.

### Catastrophic Forgetting Prevention

Add elastic weight consolidation (EWC) or replay:

```python
# Save original model predictions
original_model = GPT.from_pretrained(init_from, dict(dropout=0.0))

# Add KL divergence loss to preserve original behavior
kl_weight = 0.1
kl_loss = F.kl_div(
    F.log_softmax(logits, dim=-1),
    F.softmax(original_logits, dim=-1),
    reduction='batchmean'
)
loss = loss + kl_weight * kl_loss
```

## Hardware Considerations

<Tabs>
  <Tab title="Single GPU">
    **Recommended configs by GPU:**
    
    **A100 40GB:**
    ```python
    init_from = 'gpt2-xl'          # 1.5B params
    batch_size = 4
    gradient_accumulation_steps = 8
    ```
    
    **RTX 4090 / A6000 (24GB):**
    ```python
    init_from = 'gpt2-large'       # 774M params
    batch_size = 2
    gradient_accumulation_steps = 16
    ```
    
    **RTX 3090 / 4080 (16-24GB):**
    ```python
    init_from = 'gpt2-medium'      # 350M params
    batch_size = 2
    gradient_accumulation_steps = 16
    ```
    
    **RTX 3060 / 4060 (8-12GB):**
    ```python
    init_from = 'gpt2'             # 124M params
    batch_size = 1
    gradient_accumulation_steps = 32
    ```
  </Tab>
  
  <Tab title="Multi-GPU">
    **Distributed finetuning:**
    
    ```bash
    torchrun --standalone --nproc_per_node=4 \
      train.py config/finetune_my_dataset.py
    ```
    
    Adjust batch size accordingly:
    ```python
    # For 4 GPUs
    batch_size = 4
    gradient_accumulation_steps = 8  # Total: 4*4*8*1024 = 131K tokens/iter
    ```
  </Tab>
  
  <Tab title="CPU (Not Recommended)">
    **Only for tiny models:**
    
    ```bash
    python train.py config/finetune_my_dataset.py \
      --init_from=gpt2 \
      --device=cpu \
      --compile=False \
      --batch_size=1 \
      --block_size=512
    ```
    
    <Warning>
      Finetuning on CPU is extremely slow (hours to days) and only practical for `gpt2` (124M).
    </Warning>
  </Tab>
</Tabs>

## Evaluation and Quality Control

### Monitor Overfitting

Watch the train/val loss gap:

```
step 10: train loss 2.1234, val loss 2.1456  # Good: similar
step 20: train loss 1.8234, val loss 1.8456  # Good: both decreasing
step 30: train loss 1.5234, val loss 1.9456  # Bad: diverging (overfitting)
```

If overfitting:
- Reduce `max_iters`
- Increase `dropout`
- Use smaller model
- Get more data

### Generate Test Samples

Regularly sample during training:

```bash
# After each checkpoint
python sample.py --out_dir=out-my-dataset \
  --start="Your prompt here" \
  --num_samples=5
```

Look for:
- Domain-specific terminology
- Appropriate style/tone
- Coherent structure
- No generic GPT-2 responses

### Compare with Base Model

```bash
# Base GPT-2
python sample.py --init_from=gpt2-xl --start="Your prompt"

# Your finetuned model
python sample.py --out_dir=out-my-dataset --start="Your prompt"
```

Finetuned model should be noticeably more domain-specific.

## Common Issues

### Model Not Adapting

**Symptoms:**
- Outputs still sound like generic GPT-2
- Loss decreases slowly
- Domain-specific terms not learned

**Solutions:**
- Increase learning rate (try 1e-4)
- Train longer (more iterations)
- Check dataset quality and size
- Ensure dataset uses GPT-2 BPE tokenization

### Catastrophic Forgetting

**Symptoms:**
- Model loses general language ability
- Only generates dataset-specific text
- Grammar/coherence degrades

**Solutions:**
- Lower learning rate
- Train for fewer iterations
- Mix in general text data
- Add KL divergence regularization

### Out of Memory

**Solutions:**
- Use smaller model (gpt2 → gpt2-medium → gpt2-large → gpt2-xl)
- Reduce `batch_size`
- Reduce `block_size` (context length)
- Enable gradient checkpointing (requires code modification)

### Poor Sample Quality

**Solutions:**
- Train longer
- Increase model size
- Improve dataset quality
- Adjust sampling parameters:
  ```bash
  python sample.py --temperature=0.8 --top_k=50
  ```

## Example: Finetuning on Code

Create a code generation model:

```python
# config/finetune_code.py
import time

out_dir = 'out-code'
eval_interval = 20
eval_iters = 40

dataset = 'python_code'  # Your prepared code dataset
init_from = 'gpt2-large'  # Larger model for code

always_save_checkpoint = False

batch_size = 2
gradient_accumulation_steps = 16
max_iters = 500  # More iterations for code

learning_rate = 5e-5  # Slightly higher for code
decay_lr = True
warmup_iters = 20
lr_decay_iters = 500
min_lr = 5e-6

# Higher dropout to prevent memorization
dropout = 0.15
```

## Next Steps

<CardGroup cols={2}>
  <Card title="Training Overview" icon="book" href="/training/overview">
    Learn more about nanoGPT's training architecture
  </Card>
  
  <Card title="Distributed Training" icon="server" href="/training/distributed">
    Scale finetuning across multiple GPUs
  </Card>
</CardGroup>

## Key Takeaways

- Finetuning is 10-100× faster than training from scratch
- Use low learning rates (3e-5) to preserve pretrained knowledge
- Start with constant LR, add decay for longer runs
- Monitor train/val gap to detect overfitting
- Larger base models (gpt2-xl) generally produce better results
- Small datasets need higher dropout and fewer iterations
- Always use GPT-2 BPE tokenization, not character-level