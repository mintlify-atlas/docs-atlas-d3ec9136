---
title: Distributed Training
description: Scale training across multiple GPUs and nodes with PyTorch DDP
---

## Overview

nanoGPT supports distributed training via PyTorch's Distributed Data Parallel (DDP). This enables:

- **Multi-GPU training** on a single node
- **Multi-node training** across cluster nodes
- **Linear scaling** of throughput with GPUs
- **Efficient gradient synchronization** via NCCL

<Info>
Distributed training is automatically detected when `RANK` environment variable is set by `torchrun`. No code changes are needed - the same `train.py` works for both single-GPU and distributed training.
</Info>

## Quick Start

<Tabs>
  <Tab title="Single Node (8 GPUs)">
    ```bash
    torchrun --standalone --nproc_per_node=8 train.py
    ```
  </Tab>
  
  <Tab title="Single Node (4 GPUs)">
    ```bash
    torchrun --standalone --nproc_per_node=4 train.py
    ```
  </Tab>
  
  <Tab title="Multi-Node (2 nodes, 8 GPUs each)">
    ```bash
    # On master node (IP: 123.456.123.456)
    torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 \
        --master_addr=123.456.123.456 --master_port=1234 train.py
    
    # On worker node
    torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 \
        --master_addr=123.456.123.456 --master_port=1234 train.py
    ```
  </Tab>
</Tabs>

## Single Node Multi-GPU Setup

### Using torchrun (Recommended)

The `torchrun` utility (PyTorch 1.10+) simplifies DDP setup by automatically setting environment variables:

```bash
# Train GPT-2 on 8 GPUs
torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
```

**What `torchrun` does:**
- Sets `RANK`, `LOCAL_RANK`, `WORLD_SIZE` environment variables
- Manages process spawning and synchronization
- Handles failures and restarts gracefully

### DDP Initialization (train.py:82-95)

The training script automatically detects DDP mode:

```python
# Check if this is a DDP run
ddp = int(os.environ.get('RANK', -1)) != -1

if ddp:
    init_process_group(backend=backend)  # backend='nccl'
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    ddp_world_size = int(os.environ['WORLD_SIZE'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    master_process = ddp_rank == 0  # Only rank 0 does logging/checkpointing
    seed_offset = ddp_rank  # Each process gets different seed
    
    # Scale down gradient accumulation per process
    assert gradient_accumulation_steps % ddp_world_size == 0
    gradient_accumulation_steps //= ddp_world_size
else:
    # Single GPU mode
    master_process = True
    seed_offset = 0
    ddp_world_size = 1
```

<Note>
**Key Design Choices:**
- Only rank 0 process performs logging and checkpointing
- Each process gets a different random seed for data sampling
- Gradient accumulation steps are divided among processes
- Effective batch size remains constant regardless of world size
</Note>

### Wrapping Model with DDP

After model initialization and compilation, wrap with DDP:

```python
if ddp:
    model = DDP(model, device_ids=[ddp_local_rank])
```

### Gradient Synchronization

DDP only needs to sync gradients at the final gradient accumulation step:

```python
for micro_step in range(gradient_accumulation_steps):
    if ddp:
        # Only sync gradients on the last micro step
        model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)
    with ctx:
        logits, loss = model(X, Y)
        loss = loss / gradient_accumulation_steps
    scaler.scale(loss).backward()
```

## Multi-Node Training

### Prerequisites

<Steps>
  <Step title="Network Connectivity">
    Ensure all nodes can communicate:
    ```bash
    # Test connectivity from worker to master
    ping 123.456.123.456
    ```
  </Step>
  
  <Step title="Shared Filesystem (Optional)">
    For checkpoint sharing, mount a shared filesystem like NFS:
    ```bash
    # All nodes should access the same --out_dir
    --out_dir=/shared/nfs/nanogpt-checkpoints
    ```
  </Step>
  
  <Step title="Firewall Configuration">
    Open the master port (default: 1234) on the master node:
    ```bash
    # Example: open port 1234 TCP
    sudo ufw allow 1234/tcp
    ```
  </Step>
</Steps>

### Launch Commands

#### Master Node

Run on the first node (replace IP with your master node IP):

```bash
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 \
    --master_addr=123.456.123.456 --master_port=1234 \
    train.py config/train_gpt2.py
```

**Parameters:**
- `--nproc_per_node=8`: Number of GPUs per node
- `--nnodes=2`: Total number of nodes
- `--node_rank=0`: This is the master (rank 0)
- `--master_addr`: IP address of master node
- `--master_port`: Port for inter-node communication

#### Worker Nodes

Run on each worker node:

```bash
# Worker node 1 (node_rank=1)
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 \
    --master_addr=123.456.123.456 --master_port=1234 \
    train.py config/train_gpt2.py

# Worker node 2 (node_rank=2) if using 3 nodes
torchrun --nproc_per_node=8 --nnodes=3 --node_rank=2 \
    --master_addr=123.456.123.456 --master_port=1234 \
    train.py config/train_gpt2.py
```

<Warning>
All nodes must use identical command-line arguments (except `--node_rank`). Mismatched configs will cause training to hang or crash.
</Warning>

## NCCL Configuration

### Backend Selection

The DDP backend is configured in train.py:

```python
backend = 'nccl'  # 'nccl', 'gloo', etc.
```

**Available backends:**
- **NCCL** (default): Optimized for NVIDIA GPUs, best performance
- **Gloo**: CPU-based, works without GPUs but slower
- **MPI**: For HPC clusters with MPI support

### Infiniband Optimization

For clusters **with** Infiniband interconnect:

```bash
# NCCL will automatically use Infiniband (best performance)
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 \
    --master_addr=123.456.123.456 --master_port=1234 \
    train.py
```

For clusters **without** Infiniband:

```bash
# Disable Infiniband to avoid errors
NCCL_IB_DISABLE=1 torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 \
    --master_addr=123.456.123.456 --master_port=1234 \
    train.py
```

<Warning>
**Important:** If you don't have Infiniband, you **must** set `NCCL_IB_DISABLE=1`. Otherwise, training may hang during gradient synchronization.

Without Infiniband, multi-node training will be significantly slower due to network bottlenecks.
</Warning>

### Benchmarking Network

Test your interconnect bandwidth:

```bash
# Install iperf3
apt-get install iperf3

# On master node: start server
iperf3 -s

# On worker node: run client
iperf3 -c 123.456.123.456
```

**Expected bandwidth:**
- Infiniband: 100+ Gbps
- 10GbE network: ~10 Gbps
- 1GbE network: ~1 Gbps (too slow for multi-node)

### NCCL Environment Variables

Useful NCCL tuning options:

```bash
# Disable Infiniband
NCCL_IB_DISABLE=1

# Enable debug logging
NCCL_DEBUG=INFO

# Use specific network interface
NCCL_SOCKET_IFNAME=eth0

# Increase timeout for slow networks (in milliseconds)
NCCL_TIMEOUT=300000

# Example: run with multiple NCCL settings
NCCL_IB_DISABLE=1 NCCL_DEBUG=INFO torchrun \
    --nproc_per_node=8 --nnodes=2 --node_rank=0 \
    --master_addr=123.456.123.456 --master_port=1234 \
    train.py
```

## Scaling Efficiency

### Throughput Scaling

With efficient communication, throughput should scale linearly:

| Configuration | Tokens/sec | Speedup |
|---------------|------------|----------|
| 1 GPU | 50K | 1× |
| 8 GPUs (1 node) | 400K | 8× |
| 16 GPUs (2 nodes, IB) | 750K | 15× |
| 16 GPUs (2 nodes, 10GbE) | 500K | 10× |

<Note>
**Communication Overhead:**
- Single node: ~5% overhead (GPU-to-GPU via NVLink)
- Multi-node (Infiniband): ~10-15% overhead
- Multi-node (Ethernet): ~30-50% overhead
</Note>

### Monitoring DDP Performance

Check for synchronization bottlenecks:

```python
# Enable NCCL timing
NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=COLL torchrun ...
```

Look for:
- Long `ncclAllReduce` times: Network bottleneck
- GPU utilization drops: Waiting for gradient sync
- Uneven iteration times: Load imbalance

## Troubleshooting

<AccordionGroup>
  <Accordion title="Training hangs at initialization">
    **Causes:**
    - Firewall blocking master port
    - Wrong master IP address
    - Network connectivity issues
    
    **Solutions:**
    ```bash
    # Test connectivity
    ping <master_addr>
    telnet <master_addr> <master_port>
    
    # Check firewall
    sudo ufw status
    sudo ufw allow <master_port>/tcp
    
    # Enable NCCL debug logging
    NCCL_DEBUG=INFO torchrun ...
    ```
  </Accordion>
  
  <Accordion title="NCCL errors or hangs during training">
    **Causes:**
    - Infiniband misconfiguration
    - Network instability
    - Different CUDA versions across nodes
    
    **Solutions:**
    ```bash
    # Disable Infiniband if not available
    NCCL_IB_DISABLE=1 torchrun ...
    
    # Use TCP sockets explicitly
    NCCL_SOCKET_IFNAME=eth0 torchrun ...
    
    # Increase timeout
    NCCL_TIMEOUT=600000 torchrun ...
    ```
  </Accordion>
  
  <Accordion title="Out of memory on some GPUs">
    **Causes:**
    - Uneven data distribution
    - Rank 0 does extra work (logging)
    
    **Solutions:**
    ```bash
    # Reduce batch size slightly
    --batch_size=10  # instead of 12
    
    # Reduce gradient accumulation
    --gradient_accumulation_steps=32  # instead of 40
    ```
  </Accordion>
  
  <Accordion title="Slower than expected throughput">
    **Causes:**
    - Data loading bottleneck
    - Network bandwidth limit
    - Inefficient gradient accumulation
    
    **Solutions:**
    ```bash
    # Benchmark network
    iperf3 -c <master_addr>
    
    # Check GPU utilization
    nvidia-smi dmon -s u
    
    # Profile training
    python bench.py  # simplified training loop
    ```
  </Accordion>
</AccordionGroup>

## Best Practices

<CardGroup cols={2}>
  <Card title="Use torchrun" icon="play">
    Always use `torchrun` instead of `python -m torch.distributed.launch` (deprecated)
  </Card>
  
  <Card title="Test Single Node First" icon="flask">
    Verify training works on single node before scaling to multi-node
  </Card>
  
  <Card title="Monitor Network" icon="chart-line">
    Use `iperf3` and `NCCL_DEBUG=INFO` to identify communication bottlenecks
  </Card>
  
  <Card title="Shared Checkpoints" icon="floppy-disk">
    Use shared filesystem or cloud storage for `--out_dir` across nodes
  </Card>
</CardGroup>

## Example: 4-Node Training

Training GPT-2 on 4 nodes × 8 GPUs = 32 GPUs:

```bash
# Node 0 (master): IP 192.168.1.10
NCCL_IB_DISABLE=1 torchrun \
    --nproc_per_node=8 --nnodes=4 --node_rank=0 \
    --master_addr=192.168.1.10 --master_port=29500 \
    train.py config/train_gpt2.py --gradient_accumulation_steps=10

# Node 1 (worker): IP 192.168.1.11
NCCL_IB_DISABLE=1 torchrun \
    --nproc_per_node=8 --nnodes=4 --node_rank=1 \
    --master_addr=192.168.1.10 --master_port=29500 \
    train.py config/train_gpt2.py --gradient_accumulation_steps=10

# Node 2 (worker): IP 192.168.1.12
NCCL_IB_DISABLE=1 torchrun \
    --nproc_per_node=8 --nnodes=4 --node_rank=2 \
    --master_addr=192.168.1.10 --master_port=29500 \
    train.py config/train_gpt2.py --gradient_accumulation_steps=10

# Node 3 (worker): IP 192.168.1.13
NCCL_IB_DISABLE=1 torchrun \
    --nproc_per_node=8 --nnodes=4 --node_rank=3 \
    --master_addr=192.168.1.10 --master_port=29500 \
    train.py config/train_gpt2.py --gradient_accumulation_steps=10
```

**Result:**
- 32 GPUs total
- Gradient accumulation reduced from 40 to 10 (40 / 4 nodes)
- Effective batch size: 12 × 1024 × 10 × 32 = 3.93M tokens/iter
- Training time: ~1 day (4× speedup vs single node)

## Cleanup

After training completes:

```python
if ddp:
    destroy_process_group()  # Clean up DDP resources
```

This is automatically called at the end of train.py:336.