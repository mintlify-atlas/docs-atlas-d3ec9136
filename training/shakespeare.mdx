---
title: Character-Level Shakespeare Training
description: Train your first GPT model on Shakespeare's complete works
---

# Character-Level Shakespeare Training

This tutorial walks through training a character-level GPT on Shakespeare's works - the perfect first project to understand nanoGPT. The entire dataset is only 1MB and trains in minutes on a single GPU.

## Why Start Here?

<CardGroup cols={2}>
  <Card title="Fast Iteration" icon="bolt">
    Complete training runs in 3-5 minutes on a GPU, seconds on Apple Silicon
  </Card>
  
  <Card title="Small Dataset" icon="file">
    Just 1MB of text - no lengthy downloads or preprocessing
  </Card>
  
  <Card title="Visible Overfitting" icon="chart-line">
    See model behavior clearly on this tiny dataset
  </Card>
  
  <Card title="Fun Output" icon="theater-masks">
    Generate Shakespeare-style text immediately
  </Card>
</CardGroup>

## Quick Start

<Steps>
  <Step title="Prepare the Dataset">
    Download and prepare Shakespeare's works:
    
    ```bash
    python data/shakespeare_char/prepare.py
    ```
    
    This creates:
    - `data/shakespeare_char/train.bin` - 1,003,854 tokens
    - `data/shakespeare_char/val.bin` - 111,540 tokens
    - `data/shakespeare_char/meta.pkl` - vocabulary (65 unique characters)
  </Step>
  
  <Step title="Train the Model">
    Run training with the provided config:
    
    ```bash
    python train.py config/train_shakespeare_char.py
    ```
    
    On an A100 GPU, this takes about 3 minutes and achieves ~1.47 validation loss.
  </Step>
  
  <Step title="Generate Samples">
    Sample from your trained model:
    
    ```bash
    python sample.py --out_dir=out-shakespeare-char
    ```
  </Step>
</Steps>

## Configuration Deep Dive

Let's examine `config/train_shakespeare_char.py` line by line:

### Output and Logging

```python
out_dir = 'out-shakespeare-char'
eval_interval = 250      # frequent eval since we'll overfit
eval_iters = 200
log_interval = 10
```

- Checkpoints saved to `out-shakespeare-char/`
- Evaluate every 250 iterations (more frequent than default)
- Log training loss every 10 iterations

### Checkpointing Strategy

```python
always_save_checkpoint = False  # only save when val improves
```

<Info>
  Since this dataset is small, we expect overfitting. Only saving when validation improves prevents saving increasingly overfit checkpoints.
</Info>

### Dataset Configuration

```python
dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256  # context of up to 256 previous characters
```

- Character-level tokenization (65 unique chars)
- Context window of 256 characters
- Effective batch size: 64 × 1 × 256 = 16,384 tokens/iter

### Baby GPT Architecture

```python
n_layer = 6      # 6 transformer layers
n_head = 6       # 6 attention heads
n_embd = 384     # 384-dim embeddings
dropout = 0.2    # 20% dropout for regularization
```

This creates a small ~10M parameter model:
- 6 layers (vs. 12 for GPT-2 124M)
- 6 heads per layer (vs. 12 for GPT-2 124M)
- 384 embedding dims (vs. 768 for GPT-2 124M)

<Note>
  Higher dropout (0.2) compared to pretraining default (0.0) helps prevent overfitting on this tiny dataset.
</Note>

### Optimizer Settings

```python
learning_rate = 1e-3  # 10x higher than default
max_iters = 5000
lr_decay_iters = 5000
min_lr = 1e-4        # learning_rate / 10
beta2 = 0.99         # slightly higher for small batches
warmup_iters = 100
```

Key differences from default GPT-2 training:
- **10× higher learning rate** (1e-3 vs 6e-4): Baby networks can handle it
- **Shorter training** (5000 vs 600000 iters): Tiny dataset
- **Higher beta2** (0.99 vs 0.95): Better for small token counts

## Training on Different Hardware

<Tabs>
  <Tab title="GPU (Recommended)">
    **NVIDIA GPU with CUDA:**
    
    ```bash
    python train.py config/train_shakespeare_char.py
    ```
    
    **Expected results:**
    - Time: ~3 minutes on A100, ~10 minutes on RTX 3090
    - Validation loss: ~1.47
    - Model FLOPs Utilization (MFU): 40-50%
    
    **Output:**
    ```
    step 5000: train loss 1.4123, val loss 1.4697
    iter 5000: loss 1.4089, time 142.34ms, mfu 45.23%
    ```
  </Tab>
  
  <Tab title="Apple Silicon (M1/M2/M3)">
    **Using Metal Performance Shaders:**
    
    ```bash
    python train.py config/train_shakespeare_char.py --device=mps
    ```
    
    **Expected results:**
    - Time: ~5-10 minutes depending on chip
    - 2-3× faster than CPU
    - Similar loss to GPU training
    
    <Note>
      Make sure you have a recent PyTorch version with MPS support.
    </Note>
  </Tab>
  
  <Tab title="CPU (Minimal Setup)">
    **For machines without GPU:**
    
    ```bash
    python train.py config/train_shakespeare_char.py \
      --device=cpu \
      --compile=False \
      --eval_iters=20 \
      --log_interval=1 \
      --block_size=64 \
      --batch_size=12 \
      --n_layer=4 \
      --n_head=4 \
      --n_embd=128 \
      --max_iters=2000 \
      --lr_decay_iters=2000 \
      --dropout=0.0
    ```
    
    **Modifications for CPU:**
    - Disable `torch.compile` (not supported on CPU)
    - Reduce model size (4 layers, 128 dims)
    - Smaller context (64 vs 256)
    - Smaller batch (12 vs 64)
    - Shorter training (2000 vs 5000 iters)
    - No dropout (smaller model)
    
    **Expected results:**
    - Time: ~3 minutes
    - Validation loss: ~1.88 (higher due to smaller model)
  </Tab>
</Tabs>

## Understanding the Output

### Training Logs

```
tokens per iteration will be: 16,384
step 0: train loss 4.2341, val loss 4.2398
iter 250: loss 2.1234, time 145.23ms, mfu 43.21%
step 250: train loss 1.9876, val loss 2.0123
saving checkpoint to out-shakespeare-char
...
step 5000: train loss 1.4123, val loss 1.4697
```

Key metrics:
- **tokens per iteration**: Effective batch size in tokens
- **train/val loss**: Cross-entropy loss (lower is better)
- **mfu**: GPU utilization percentage
- **time**: Milliseconds per iteration

### Generated Samples

After training, generate samples:

```bash
python sample.py --out_dir=out-shakespeare-char
```

**Example output:**

```
ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang'd
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?
```

Not perfect, but remarkably Shakespearean for 3 minutes of training!

## Advanced Customization

### Adjust Model Size

Create larger or smaller models:

```bash
# Tiny model (~2M params) - faster training
python train.py config/train_shakespeare_char.py \
  --n_layer=4 --n_head=4 --n_embd=256

# Large model (~40M params) - better results
python train.py config/train_shakespeare_char.py \
  --n_layer=12 --n_head=12 --n_embd=768
```

### Modify Context Length

```bash
# Shorter context (faster, less memory)
python train.py config/train_shakespeare_char.py --block_size=128

# Longer context (more coherent text)
python train.py config/train_shakespeare_char.py --block_size=512
```

<Warning>
  Longer context requires more GPU memory. Reduce `batch_size` if you run out of memory.
</Warning>

### Train Longer

```bash
python train.py config/train_shakespeare_char.py \
  --max_iters=10000 \
  --lr_decay_iters=10000
```

### Enable Weights & Biases Logging

```bash
python train.py config/train_shakespeare_char.py \
  --wandb_log=True \
  --wandb_project=shakespeare \
  --wandb_run_name=my-first-gpt
```

## Sampling Options

### Custom Prompts

```bash
# Prompt from command line
python sample.py --out_dir=out-shakespeare-char \
  --start="ROMEO:" \
  --num_samples=5 \
  --max_new_tokens=200

# Prompt from file
echo "JULIET:" > prompt.txt
python sample.py --out_dir=out-shakespeare-char \
  --start=FILE:prompt.txt
```

### Control Randomness

```bash
# More deterministic (less creative)
python sample.py --out_dir=out-shakespeare-char \
  --temperature=0.5 --top_k=50

# More random (more creative)
python sample.py --out_dir=out-shakespeare-char \
  --temperature=1.2 --top_k=500
```

- **temperature**: Lower = more deterministic, higher = more random
- **top_k**: Only sample from top K most likely tokens

## Troubleshooting

### Out of Memory

Reduce memory usage:

```bash
python train.py config/train_shakespeare_char.py \
  --batch_size=32 \
  --block_size=128
```

### Slow Training

Ensure torch.compile is enabled:

```bash
python train.py config/train_shakespeare_char.py --compile=True
```

### Poor Results

Train longer or use a larger model:

```bash
python train.py config/train_shakespeare_char.py \
  --max_iters=10000 \
  --n_layer=8 --n_head=8 --n_embd=512
```

## Next Steps

<CardGroup cols={2}>
  <Card title="GPT-2 Reproduction" icon="robot" href="/training/gpt2-reproduction">
    Scale up to reproduce GPT-2 on OpenWebText
  </Card>
  
  <Card title="Finetuning" icon="sliders" href="/training/finetuning">
    Finetune a pretrained GPT-2 on Shakespeare
  </Card>
</CardGroup>

## Key Takeaways

- Character-level models are perfect for learning and experimentation
- Small datasets require higher dropout and careful regularization
- Baby networks can use higher learning rates
- Training from scratch on tiny data takes just minutes
- Generated samples show the model learned Shakespeare's style