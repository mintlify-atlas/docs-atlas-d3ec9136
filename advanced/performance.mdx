---
title: 'Performance Optimization'
description: 'Optimize training speed and memory efficiency in nanoGPT'
icon: 'gauge'
---

## Overview

nanoGPT is optimized for maximum training performance on modern GPUs. This guide covers the key optimizations implemented in the codebase and how to leverage them for faster training.

## PyTorch 2.0 Compilation

The most significant performance boost comes from PyTorch 2.0's `torch.compile()` feature.

### Enabling Compilation

```python
# In train.py (line 74)
compile = True  # use PyTorch 2.0 to compile the model to be faster
```

The compilation happens after model initialization:

```python
# From train.py:204-208
if compile:
    print("compiling the model... (takes a ~minute)")
    unoptimized_model = model
    model = torch.compile(model)  # requires PyTorch 2.0
```

<Note>
The first iteration after compilation will be slow as PyTorch analyzes and optimizes the computation graph. Subsequent iterations will be significantly faster.
</Note>

### Performance Impact

Compilation can provide **1.5-2x speedup** depending on your hardware and model size. From `bench.py:19`:

```python
compile = True  # use PyTorch 2.0 to compile the model to be faster
```

## Flash Attention

Flash Attention is an optimized attention mechanism that reduces memory usage and computation time.

### Automatic Detection

From `model.py:44-50`, nanoGPT automatically detects and uses Flash Attention if available:

```python
# flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0
self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
if not self.flash:
    print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")
    # causal mask to ensure that attention is only applied to the left in the input sequence
    self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                .view(1, 1, config.block_size, config.block_size))
```

### Implementation

When Flash Attention is available (model.py:62-64):

```python
if self.flash:
    # efficient attention using Flash Attention CUDA kernels
    y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
```

<Tip>
Flash Attention provides:
- **2-4x speedup** for attention computation
- Reduced memory usage allowing larger batch sizes
- Available in PyTorch >= 2.0
</Tip>

## TensorFloat-32 (TF32) Precision

For NVIDIA A100 GPUs, TF32 provides significant speedup with minimal accuracy loss.

### Enabling TF32

From `train.py:107-108`:

```python
torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul
torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn
```

This is also enabled in `bench.py:26-27`:

```python
torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul
torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn
```

<Warning>
TF32 is only available on NVIDIA Ampere GPUs (A100, A6000, etc.). On other GPUs, these settings are ignored.
</Warning>

### TF32 vs Other Precision Modes

<Tabs>
  <Tab title="FP32">
    **Full Precision (32-bit)**
    - Highest accuracy
    - Slowest training
    - Highest memory usage
    - Use for reference or debugging
  </Tab>
  <Tab title="TF32">
    **TensorFloat-32**
    - ~1.5-2x faster than FP32 on A100
    - Minimal accuracy loss
    - No code changes needed
    - Default for A100 GPUs
  </Tab>
  <Tab title="FP16">
    **Half Precision (16-bit)**
    - 2-3x faster than FP32
    - Requires gradient scaling
    - May need careful tuning
    - Good for inference
  </Tab>
  <Tab title="BF16">
    **BFloat16**
    - Better numerical stability than FP16
    - Native A100 support
    - Recommended for training
    - Selected automatically when available
  </Tab>
</Tabs>

## Mixed Precision Training

From `train.py:73`:

```python
dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'
```

### Automatic Mixed Precision Setup

```python
# From train.py:111-112
ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]
ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)
```

### Gradient Scaling for FP16

From `train.py:196`:

```python
# initialize a GradScaler. If enabled=False scaler is a no-op
scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
```

## Benchmarking with bench.py

The `bench.py` script provides a streamlined way to measure performance.

### Running Benchmarks

```bash
# Basic benchmark
python bench.py

# With profiling
python bench.py --profile=True

# Without compilation
python bench.py --compile=False
```

### MFU Calculation

From `bench.py:115-117`, the script calculates Model FLOPs Utilization:

```python
mfu = model.estimate_mfu(batch_size * 1 * num_steps, dt)
if stage == 1:
    print(f"time per iteration: {dt/num_steps*1000:.4f}ms, MFU: {mfu*100:.2f}%")
```

### Understanding MFU

Model FLOPs Utilization (MFU) measures how efficiently your training uses the GPU's theoretical peak performance.

From `model.py:289-303`:

```python
def estimate_mfu(self, fwdbwd_per_iter, dt):
    """ estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS """
    # first estimate the number of flops we do per iteration.
    # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311
    N = self.get_num_params()
    cfg = self.config
    L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size
    flops_per_token = 6*N + 12*L*H*Q*T
    flops_per_fwdbwd = flops_per_token * T
    flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter
    # express our flops throughput as ratio of A100 bfloat16 peak flops
    flops_achieved = flops_per_iter * (1.0/dt)  # per second
    flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS
    mfu = flops_achieved / flops_promised
    return mfu
```

<Note>
**Typical MFU values:**
- 40-50%: Good utilization
- 50-60%: Excellent utilization
- 60%+: Outstanding (rare without significant optimization)
</Note>

## Memory Optimizations

### Gradient Accumulation

From `train.py:48`:

```python
gradient_accumulation_steps = 5 * 8  # used to simulate larger batch sizes
```

This allows training with effective batch sizes larger than what fits in GPU memory:

```python
# From train.py:101
tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size
```

### Efficient Data Loading

From `train.py:117-122`, nanoGPT uses memory-mapped files to avoid memory leaks:

```python
def get_batch(split):
    # We recreate np.memmap every batch to avoid a memory leak, as per
    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122
    if split == 'train':
        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')
    else:
        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')
```

### Pin Memory for Faster Transfers

From `train.py:126-130`:

```python
if device_type == 'cuda':
    # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)
    x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)
else:
    x, y = x.to(device), y.to(device)
```

## Distributed Training

### DDP Setup

From `train.py:82-95`:

```python
ddp = int(os.environ.get('RANK', -1)) != -1  # is this a ddp run?
if ddp:
    init_process_group(backend=backend)
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    ddp_world_size = int(os.environ['WORLD_SIZE'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    master_process = ddp_rank == 0  # this process will do logging, checkpointing etc.
    seed_offset = ddp_rank  # each process gets a different seed
    # world_size number of processes will be training simultaneously, so we can scale
    # down the desired gradient accumulation iterations per process proportionally
    assert gradient_accumulation_steps % ddp_world_size == 0
    gradient_accumulation_steps //= ddp_world_size
```

### Launch Multi-GPU Training

```bash
# Single node, 4 GPUs
torchrun --standalone --nproc_per_node=4 train.py

# Multi-node training (8 GPUs per node, 2 nodes)
# Master node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 \
  --master_addr=123.456.123.456 --master_port=1234 train.py

# Worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 \
  --master_addr=123.456.123.456 --master_port=1234 train.py
```

<Note>
From train.py:16, for InfiniBand clusters:
```bash
NCCL_IB_DISABLE=1 torchrun ...
```
</Note>

## Performance Checklist

<Tabs>
  <Tab title="Software">
    - [ ] PyTorch >= 2.0 installed
    - [ ] `compile=True` enabled
    - [ ] Flash Attention available
    - [ ] Appropriate dtype selected (bfloat16 recommended)
    - [ ] TF32 enabled (A100 GPUs)
  </Tab>
  <Tab title="Hardware">
    - [ ] Using modern GPUs (A100, V100, etc.)
    - [ ] Fast storage for dataset (NVMe SSD)
    - [ ] Sufficient CPU-GPU bandwidth
    - [ ] Adequate CPU cores for data loading
  </Tab>
  <Tab title="Configuration">
    - [ ] Batch size maximized for GPU memory
    - [ ] Gradient accumulation configured
    - [ ] Pin memory enabled
    - [ ] Async data transfers enabled
    - [ ] Efficient gradient synchronization (DDP)
  </Tab>
</Tabs>

## Troubleshooting Performance

### Low MFU (&lt;30%)

1. **Check GPU utilization**: `nvidia-smi dmon`
2. **Increase batch size**: If GPU memory allows
3. **Enable compilation**: Set `compile=True`
4. **Check data loading**: Ensure it's not a bottleneck

### Out of Memory Errors

1. **Reduce batch size**: Start with `batch_size=1`
2. **Increase gradient accumulation**: Maintain effective batch size
3. **Use smaller model**: Reduce `n_embd`, `n_layer`, or `n_head`
4. **Enable gradient checkpointing**: Trade compute for memory

### Slow Compilation

<Warning>
The first training iteration after `torch.compile()` can take 1-2 minutes. This is normal and only happens once.
</Warning>

## Performance References

- [PyTorch 2.0 Documentation](https://pytorch.org/get-started/pytorch-2.0/)
- [Flash Attention Paper](https://arxiv.org/abs/2205.14135)
- [PaLM Paper (Appendix B - FLOP calculation)](https://arxiv.org/abs/2204.02311)
- [NVIDIA A100 Specifications](https://www.nvidia.com/en-us/data-center/a100/)