---
title: 'Hyperparameter Tuning'
description: 'Comprehensive guide to training hyperparameters in nanoGPT'
icon: 'sliders'
---

## Overview

This guide covers all major hyperparameters in nanoGPT and provides practical advice for tuning them. All parameters shown are from the actual `train.py` source code.

## Model Architecture Parameters

From `train.py:51-56`:

```python
# model
n_layer = 12
n_head = 12
n_embd = 768
dropout = 0.0  # for pretraining 0 is good, for finetuning try 0.1+
bias = False  # do we use bias inside LayerNorm and Linear layers?
```

### n_layer (Number of Transformer Blocks)

**Default**: 12 (GPT-2 small)

<Tabs>
  <Tab title="Effects">
    - Controls model depth
    - More layers = more capacity
    - Diminishing returns after ~24-48 layers
    - Memory usage: Linear with n_layer
  </Tab>
  <Tab title="Recommendations">
    - **Small models (&lt;500M)**: 12-24 layers
    - **Medium models (500M-2B)**: 24-36 layers
    - **Large models (&gt;2B)**: 36-48+ layers
    - Always use multiples of 12 for better GPU utilization
  </Tab>
</Tabs>

### n_head (Number of Attention Heads)

**Default**: 12

<Warning>
`n_embd` must be divisible by `n_head`. Each head dimension is `n_embd // n_head`.
</Warning>

```python
# From model.py:31-33
assert config.n_embd % config.n_head == 0
# key, query, value projections for all heads, but in a batch
self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
```

**Typical configurations**:
- n_embd=768 → n_head=12 (head_dim=64)
- n_embd=1024 → n_head=16 (head_dim=64)
- n_embd=1280 → n_head=20 (head_dim=64)
- n_embd=1600 → n_head=25 (head_dim=64)

<Tip>
**Best Practice**: Keep head dimension at 64 or 128. Don't go below 32 or above 128.
</Tip>

### n_embd (Embedding Dimension)

**Default**: 768

Controls model width. From `model.py:82`:

```python
class MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)  # expand to 4x
        # ...
```

<Note>
**MLP Hidden Size**: Always 4× embedding dimension in GPT architecture.
</Note>

**Standard sizes**:
- 768: GPT-2 small (124M params)
- 1024: GPT-2 medium (350M params)  
- 1280: GPT-2 large (774M params)
- 1600: GPT-2 XL (1.5B params)

### dropout

**Default**: 0.0

From `train.py:55`:

```python
dropout = 0.0  # for pretraining 0 is good, for finetuning try 0.1+
```

<Tabs>
  <Tab title="Pretraining">
    **Recommended: 0.0**
    - Large datasets don't need dropout
    - Adds unnecessary regularization
    - Slows down training
  </Tab>
  <Tab title="Finetuning">
    **Recommended: 0.1 - 0.2**
    - Prevents overfitting on small datasets
    - Start with 0.1, increase if overfitting
    - Monitor train/val loss gap
  </Tab>
</Tabs>

### bias

**Default**: False

From `train.py:56` and `model.py:116`:

```python
bias = False  # do we use bias inside LayerNorm and Linear layers?
```

<Tip>
**Recommendation**: Use `bias=False`
- Slightly better performance
- Fewer parameters (~1-2% reduction)
- Faster training
- This is the modern default
</Tip>

## Optimization Parameters

### Learning Rate

From `train.py:58`:

```python
learning_rate = 6e-4  # max learning rate
```

**Range**: 3e-4 to 1e-3 for pretraining

<Tabs>
  <Tab title="Small Models">
    **(&lt;500M params)**
    - 6e-4 to 1e-3
    - Can handle higher LR
    - Trains faster
  </Tab>
  <Tab title="Large Models">
    **(&gt;500M params)**
    - 3e-4 to 6e-4
    - Need lower LR for stability
    - More sensitive to LR
  </Tab>
  <Tab title="Finetuning">
    - 1e-5 to 1e-4
    - Much lower than pretraining
    - Prevents catastrophic forgetting
  </Tab>
</Tabs>

### Learning Rate Schedule

From `train.py:64-68`:

```python
# learning rate decay settings
decay_lr = True  # whether to decay the learning rate
warmup_iters = 2000  # how many steps to warm up for
lr_decay_iters = 600000  # should be ~= max_iters per Chinchilla
min_lr = 6e-5  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
```

#### Schedule Implementation

From `train.py:231-242`:

```python
def get_lr(it):
    # 1) linear warmup for warmup_iters steps
    if it < warmup_iters:
        return learning_rate * (it + 1) / (warmup_iters + 1)
    # 2) if it > lr_decay_iters, return min learning rate
    if it > lr_decay_iters:
        return min_lr
    # 3) in between, use cosine decay down to min learning rate
    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
    assert 0 <= decay_ratio <= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1
    return min_lr + coeff * (learning_rate - min_lr)
```

<Note>
**Three-Phase Schedule**:
1. **Linear warmup** (0 → warmup_iters): Gradually increase to max LR
2. **Cosine decay** (warmup_iters → lr_decay_iters): Smooth decay to min LR
3. **Constant** (> lr_decay_iters): Stay at min LR
</Note>

#### warmup_iters

**Default**: 2000

<Tip>
**Recommendations**:
- Small models: 1000-2000 steps
- Large models: 2000-4000 steps  
- Very large models (>10B): 5000+ steps
- ~1-2% of total training steps
</Tip>

#### min_lr

**Default**: 6e-5 (learning_rate / 10)

<Warning>
From train.py:68, Chinchilla recommends:
```python
min_lr = 6e-5  # should be ~= learning_rate/10 per Chinchilla
```
</Warning>

### AdamW Optimizer

From `train.py:58-63`:

```python
# adamw optimizer
learning_rate = 6e-4
max_iters = 600000
weight_decay = 1e-1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0
```

#### Optimizer Configuration

From `model.py:263-287`, nanoGPT uses selective weight decay:

```python
def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):
    # start with all of the candidate parameters
    param_dict = {pn: p for pn, p in self.named_parameters()}
    # filter out those that do not require grad
    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}
    # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.
    # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.
    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]
    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]
    optim_groups = [
        {'params': decay_params, 'weight_decay': weight_decay},
        {'params': nodecay_params, 'weight_decay': 0.0}
    ]
    num_decay_params = sum(p.numel() for p in decay_params)
    num_nodecay_params = sum(p.numel() for p in nodecay_params)
    print(f"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters")
    print(f"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters")
    # Create AdamW optimizer and use the fused version if it is available
    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters
    use_fused = fused_available and device_type == 'cuda'
    extra_args = dict(fused=True) if use_fused else dict()
    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)
    print(f"using fused AdamW: {use_fused}")
    return optimizer
```

<Note>
**Key Insight**: Only 2D parameters (weight matrices) get weight decay. 1D parameters (biases, LayerNorm) don't.
</Note>

#### weight_decay

**Default**: 0.1

- **Pretraining**: 0.1 (standard)
- **Finetuning**: 0.01 - 0.1 (lower for small datasets)
- Prevents overfitting on weight matrices

#### beta1 and beta2

**Defaults**: beta1=0.9, beta2=0.95

```python
# From train.py:199
optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)
```

<Tip>
These are near-universal constants. Only change if you have a good reason:
- **beta1=0.9**: Momentum term
- **beta2=0.95**: Second moment (variance) term
- Lower beta2 (0.95 vs 0.999) is better for transformers
</Tip>

#### grad_clip

**Default**: 1.0

From `train.py:307-309`:

```python
# clip the gradient
if grad_clip != 0.0:
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
```

- **1.0**: Standard for stable training
- **0.0**: Disable gradient clipping
- Increase to 2.0-5.0 if seeing gradient exploding

## Batch Size and Accumulation

From `train.py:48-50`:

```python
gradient_accumulation_steps = 5 * 8  # used to simulate larger batch sizes
batch_size = 12  # if gradient_accumulation_steps > 1, this is the micro-batch size
block_size = 1024
```

### Effective Batch Size

From `train.py:101`:

```python
tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size
print(f"tokens per iteration will be: {tokens_per_iter:,}")
```

**Formula**:
```
effective_batch_size = micro_batch_size × gradient_accumulation_steps × num_gpus
tokens_per_iter = effective_batch_size × block_size
```

### Example Calculation

From `config/train_gpt2.py:9-13`:

```python
# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8
```

<Note>
**Tokens per iteration**: 12 × 1024 × 40 = 491,520 tokens/step

This is the recommended batch size for GPT-2 training.
</Note>

### batch_size (Micro-batch Size)

**Default**: 12

<Tabs>
  <Tab title="Setting batch_size">
    - Start with largest that fits in GPU memory
    - Typical values: 4-32
    - Smaller for larger models
    - Use gradient accumulation for larger effective batches
  </Tab>
  <Tab title="A100 40GB">
    - GPT-2 Small (124M): 16-32
    - GPT-2 Medium (350M): 8-16  
    - GPT-2 Large (774M): 4-8
    - GPT-2 XL (1.5B): 2-4
  </Tab>
  <Tab title="Performance">
    - Larger micro-batches = better GPU utilization
    - Too large = OOM error
    - Too small = slow training
    - Sweet spot usually 8-16
  </Tab>
</Tabs>

### gradient_accumulation_steps

**Default**: 40 (5 × 8)

From `train.py:94-95`:

```python
# world_size number of processes will be training simultaneously, so we can scale
# down the desired gradient accumulation iterations per process proportionally
assert gradient_accumulation_steps % ddp_world_size == 0
gradient_accumulation_steps //= ddp_world_size
```

<Warning>
With DDP, gradient accumulation is automatically divided by world size. Set it to your target value × num_gpus.
</Warning>

### block_size (Context Length)

**Default**: 1024

From `train.py:50`:

```python
block_size = 1024
```

- GPT-2: 1024 tokens
- Longer contexts = more memory
- Attention cost is O(n²)

<Tip>
**Memory vs Context Tradeoff**:
- 512: 1/4 memory of 1024
- 1024: Standard, good balance
- 2048: 4× memory, may need smaller batch
- 4096: 16× memory, specialized hardware needed
</Tip>

## Training Duration

From `train.py:59`:

```python
max_iters = 600000  # total number of training iterations
```

### Calculating Total Tokens

From `config/train_gpt2.py:15-17`:

```python
# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000
```

**Formula**:
```
total_tokens = max_iters × tokens_per_iter
             = 600,000 × 491,520
             = 294.9B ≈ 300B tokens
```

<Note>
For compute-optimal training (Chinchilla), see the [Scaling Laws](/advanced/scaling-laws) guide.
</Note>

## Data Parameters

From `train.py:47`:

```python
dataset = 'openwebtext'
```

### Preparing Your Dataset

Your dataset should be prepared as binary files:
- `train.bin`: Training data
- `val.bin`: Validation data  
- `meta.pkl`: Metadata (vocab_size, etc.)

From `train.py:138-144`:

```python
# attempt to derive vocab_size from the dataset
meta_path = os.path.join(data_dir, 'meta.pkl')
meta_vocab_size = None
if os.path.exists(meta_path):
    with open(meta_path, 'rb') as f:
        meta = pickle.load(f)
    meta_vocab_size = meta['vocab_size']
    print(f"found vocab_size = {meta_vocab_size} (inside {meta_path})")
```

## Evaluation and Logging

From `train.py:36-40`:

```python
eval_interval = 2000
log_interval = 1
eval_iters = 200
eval_only = False
always_save_checkpoint = True
```

### eval_interval

**Default**: 2000

How often to evaluate on validation set:
- **2000**: Good balance (every ~1 hour for GPT-2)
- **1000**: More frequent monitoring
- **5000**: Less overhead for long runs

### eval_iters

**Default**: 200

From `train.py:215-228`:

```python
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            with ctx:
                logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out
```

Number of batches to average for loss estimation:
- **200**: Good estimate, reasonable time
- **500**: More accurate, slower
- **100**: Faster, noisier estimates

## Initialization Parameters

From `train.py:41`:

```python
init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'
```

<Tabs>
  <Tab title="'scratch'">
    **Train from random initialization**
    ```python
    init_from = 'scratch'
    ```
    - For pretraining
    - Requires large dataset
    - Full training run
  </Tab>
  <Tab title="'resume'">
    **Resume from checkpoint**
    ```python
    init_from = 'resume'
    ```
    - Continue interrupted training
    - Loads model, optimizer, iteration count
    - From `out_dir/ckpt.pt`
  </Tab>
  <Tab title="'gpt2*'">
    **Start from OpenAI GPT-2**
    ```python
    init_from = 'gpt2'        # 124M
    init_from = 'gpt2-medium' # 350M
    init_from = 'gpt2-large'  # 774M
    init_from = 'gpt2-xl'     # 1.5B
    ```
    - For finetuning
    - Faster convergence
    - Better for small datasets
  </Tab>
</Tabs>

## System Parameters

From `train.py:70-74`:

```python
# DDP settings
backend = 'nccl'  # 'nccl', 'gloo', etc.
# system  
device = 'cuda'
dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'
compile = True
```

### backend

- **'nccl'**: Best for multi-GPU (NVIDIA)
- **'gloo'**: CPU or non-NVIDIA GPUs

### dtype

<Tabs>
  <Tab title="bfloat16">
    **Recommended for A100**
    - Best numerical stability
    - Native A100 support
    - No gradient scaling needed
    - Slightly slower on V100
  </Tab>
  <Tab title="float16">
    **For V100/T4**
    - Faster on older GPUs
    - Requires gradient scaling
    - May need careful tuning
    - Good fallback option
  </Tab>
  <Tab title="float32">
    **Debugging only**
    - Highest precision
    - 2-3× slower
    - 2× memory usage
    - Use only for debugging
  </Tab>
</Tabs>

### compile

**Default**: True

```python
compile = True  # use PyTorch 2.0 to compile the model to be faster
```

<Warning>
Requires PyTorch >= 2.0. Provides 1.5-2× speedup. See [Performance Guide](/advanced/performance) for details.
</Warning>

## Weights & Biases Integration

From `train.py:43-45`:

```python
wandb_log = False  # disabled by default
wandb_project = 'owt'
wandb_run_name = 'gpt2'  # 'run' + str(time.time())
```

### Enabling W&B

```python
wandb_log = True
wandb_project = 'my-gpt-project'
wandb_run_name = 'gpt2-124M-run1'
```

Logged metrics (train.py:267-273):

```python
if wandb_log:
    wandb.log({
        "iter": iter_num,
        "train/loss": losses['train'],
        "val/loss": losses['val'],
        "lr": lr,
        "mfu": running_mfu*100,  # convert to percentage
    })
```

## Hyperparameter Tuning Strategy

### 1. Start with Defaults

The nanoGPT defaults are well-chosen. Start here and only change what's necessary.

### 2. Priority Order

Tune in this order:

1. **Model size** (n_layer, n_head, n_embd) - Based on compute budget
2. **Batch size** - Maximize GPU utilization  
3. **Learning rate** - Most impactful optimizer param
4. **Training duration** - Based on scaling laws
5. **LR schedule** - Fine-tune warmup/decay
6. **Weight decay** - If overfitting occurs
7. **Dropout** - Only for finetuning

### 3. Diagnostic Guidelines

<Tabs>
  <Tab title="Training too slow">
    - Enable `compile=True`
    - Check Flash Attention is working
    - Increase `batch_size` if memory allows
    - Enable TF32 on A100
    - Reduce `eval_interval`
  </Tab>
  <Tab title="Loss not decreasing">
    - Increase learning rate
    - Check learning rate schedule
    - Increase model size
    - Train longer
    - Verify data quality
  </Tab>
  <Tab title="Loss diverging">
    - Decrease learning rate
    - Increase warmup_iters
    - Enable gradient clipping
    - Check for data corruption
    - Reduce batch size
  </Tab>
  <Tab title="Overfitting">
    - Increase weight_decay
    - Add dropout (finetuning)
    - Train on more data
    - Reduce model size
    - Early stopping
  </Tab>
</Tabs>

### 4. Quick Start Configs

<Tabs>
  <Tab title="Quick Test (1 GPU)">
    ```python
    # Fast iteration for debugging
    batch_size = 4
    gradient_accumulation_steps = 1
    max_iters = 1000
    eval_interval = 100
    n_layer = 6
    n_head = 6  
    n_embd = 384
    ```
  </Tab>
  <Tab title="Small Scale (1-2 GPUs)">
    ```python
    # ~100M params
    batch_size = 12
    gradient_accumulation_steps = 10
    max_iters = 50000
    n_layer = 12
    n_head = 12
    n_embd = 768
    learning_rate = 6e-4
    ```
  </Tab>
  <Tab title="Production (8 GPUs)">
    ```python
    # GPT-2 (124M)
    batch_size = 12
    gradient_accumulation_steps = 40
    max_iters = 600000
    n_layer = 12
    n_head = 12
    n_embd = 768  
    learning_rate = 6e-4
    # See config/train_gpt2.py
    ```
  </Tab>
</Tabs>

## Configuration Files

nanoGPT supports config files for easier hyperparameter management.

From `train.py:77`:

```python
exec(open('configurator.py').read())  # overrides from command line or config file
```

### Using Config Files

```bash
# Create a config file
cat > config/my_config.py << EOF
batch_size = 16
learning_rate = 3e-4
max_iters = 100000
EOF

# Run with config
python train.py config/my_config.py
```

### Example: Finetuning Config

From `config/finetune_shakespeare.py` structure:

```python
# Finetuning config
init_from = 'gpt2'
batch_size = 8
block_size = 512
max_iters = 5000

# Finetuning-specific
learning_rate = 1e-4
weight_decay = 0.01
dropout = 0.1

# Faster eval
eval_interval = 250
eval_iters = 50
```

## Summary: Default Configuration

Here's the complete default configuration for GPT-2 (124M) training:

```python
# Model architecture
n_layer = 12
n_head = 12  
n_embd = 768
dropout = 0.0
bias = False
block_size = 1024

# Optimization
learning_rate = 6e-4
weight_decay = 1e-1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0

# Learning rate schedule
decay_lr = True
warmup_iters = 2000
lr_decay_iters = 600000
min_lr = 6e-5

# Batch size
batch_size = 12
gradient_accumulation_steps = 40

# Training duration  
max_iters = 600000  # 300B tokens

# System
device = 'cuda'
dtype = 'bfloat16'
compile = True

# Evaluation
eval_interval = 2000
eval_iters = 200
log_interval = 1
```

This configuration trains GPT-2 (124M) to a loss of ~2.85 on OpenWebText in ~5 days on 8× A100 GPUs.