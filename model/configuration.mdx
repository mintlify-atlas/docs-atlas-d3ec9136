---
title: 'Model Configuration'
description: 'GPTConfig and model configuration parameters'
---

## Overview

The `GPTConfig` dataclass defines all hyperparameters needed to instantiate a GPT model. It provides sensible defaults based on the original GPT-2 architecture.

## GPTConfig

```python
@dataclass
class GPTConfig:
    block_size: int = 1024
    vocab_size: int = 50304
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    dropout: float = 0.0
    bias: bool = True
```

## Configuration Parameters

### block_size

<ParamField path="block_size" type="int" default="1024">
  Maximum sequence length (context window) that the model can process. This defines the size of the position embedding table.
</ParamField>

**Details:**
- Determines the maximum number of tokens the model can attend to
- Longer sequences require more memory (quadratic in attention)
- Standard GPT-2 uses 1024
- Can be reduced for smaller, faster models

```python
# Create a model with smaller context window
config = GPTConfig(block_size=256)
model = GPT(config)
```

<Note>
If you load a pretrained model with `block_size=1024` but want to use a smaller window, use `model.crop_block_size(new_size)` to adjust it.
</Note>

### vocab_size

<ParamField path="vocab_size" type="int" default="50304">
  Size of the vocabulary (number of unique tokens). Default is 50304, which is GPT-2's 50257 padded to the nearest multiple of 64 for efficiency.
</ParamField>

**Details:**
- Must match your tokenizer's vocabulary size
- Padded to multiples of 64 for GPU efficiency
- GPT-2 tokenizer: 50257 tokens (padded to 50304)
- Affects embedding table size and output projection

```python
# GPT-2 vocab_size
config = GPTConfig(vocab_size=50257)  # Exact GPT-2 size
config = GPTConfig(vocab_size=50304)  # Padded for efficiency (default)

# Custom vocabulary
config = GPTConfig(vocab_size=32000)  # Smaller custom vocab
```

<Info>
Padding vocabulary size to multiples of 64 (or 128) can improve GPU performance without significantly increasing memory usage.
</Info>

### n_layer

<ParamField path="n_layer" type="int" default="12">
  Number of transformer blocks (layers) in the model. More layers increase model capacity and parameter count.
</ParamField>

**Details:**
- Standard GPT-2: 12 layers (124M params)
- GPT-2 Medium: 24 layers
- GPT-2 Large: 36 layers
- GPT-2 XL: 48 layers

```python
# Smaller model for experimentation
config = GPTConfig(n_layer=6)   # ~60M params

# Larger models
config = GPTConfig(n_layer=24)  # GPT-2 Medium scale
```

### n_head

<ParamField path="n_head" type="int" default="12">
  Number of attention heads in multi-head attention. Must evenly divide `n_embd`.
</ParamField>

**Details:**
- Each head processes `n_embd / n_head` dimensions
- More heads allow attending to different representation subspaces
- Must satisfy: `n_embd % n_head == 0`

```python
# Standard configurations
config = GPTConfig(n_embd=768, n_head=12)   # head_dim = 64
config = GPTConfig(n_embd=1024, n_head=16)  # head_dim = 64
config = GPTConfig(n_embd=768, n_head=8)    # head_dim = 96
```

<Note>
The head dimension (`n_embd / n_head`) is typically 64 in most GPT models, which balances expressiveness and efficiency.
</Note>

### n_embd

<ParamField path="n_embd" type="int" default="768">
  Embedding dimension (hidden size). This is the size of the hidden representations throughout the model.
</ParamField>

**Details:**
- Determines the width of the model
- Affects all linear layers and embeddings
- Standard GPT-2: 768
- Larger models use 1024, 1280, 1600

```python
# Smaller model
config = GPTConfig(n_embd=512, n_head=8)

# GPT-2 Medium scale
config = GPTConfig(n_embd=1024, n_head=16)
```

**Relationship with other parameters:**
- MLP hidden dimension: `4 * n_embd`
- Attention head dimension: `n_embd / n_head`
- Total parameters ≈ `12 * n_layer * n_embd²`

### dropout

<ParamField path="dropout" type="float" default="0.0">
  Dropout probability applied throughout the model for regularization. 0.0 means no dropout.
</ParamField>

**Details:**
- Applied in attention, residual connections, and MLP
- Common values: 0.0 (no dropout), 0.1, 0.2
- GPT-2 was trained without dropout (0.0)
- Can help prevent overfitting on smaller datasets

```python
# No dropout (default, good for large datasets)
config = GPTConfig(dropout=0.0)

# With dropout (good for smaller datasets)
config = GPTConfig(dropout=0.1)
```

<Info>
For large-scale pretraining (like GPT-2), dropout is typically 0.0. For fine-tuning on smaller datasets, values like 0.1 can help prevent overfitting.
</Info>

### bias

<ParamField path="bias" type="bool" default="True">
  Whether to use bias terms in Linear layers and LayerNorm. False can be slightly faster and uses less memory.
</ParamField>

**Details:**
- `True`: Include bias in all Linear and LayerNorm layers (GPT-2 style)
- `False`: No bias terms (slight speedup and memory savings)
- Pretrained GPT-2 models require `bias=True`

```python
# With bias (GPT-2 compatible)
config = GPTConfig(bias=True)

# Without bias (faster, modern approach)
config = GPTConfig(bias=False)
```

<Note>
Modern architectures often use `bias=False`. The original GPT-2 used `bias=True`, so pretrained checkpoints require this setting.
</Note>

## Preset Configurations

### GPT-2 Model Sizes

When loading pretrained models, these configurations are automatically set:

<Tabs>
  <Tab title="GPT-2 (124M)">
    ```python
    config = GPTConfig(
        n_layer=12,
        n_head=12,
        n_embd=768,
        vocab_size=50257,
        block_size=1024,
        bias=True,
        dropout=0.0
    )
    model = GPT.from_pretrained('gpt2')
    ```
    
    **Parameters:** 124M
  </Tab>
  
  <Tab title="GPT-2 Medium (350M)">
    ```python
    config = GPTConfig(
        n_layer=24,
        n_head=16,
        n_embd=1024,
        vocab_size=50257,
        block_size=1024,
        bias=True,
        dropout=0.0
    )
    model = GPT.from_pretrained('gpt2-medium')
    ```
    
    **Parameters:** 350M
  </Tab>
  
  <Tab title="GPT-2 Large (774M)">
    ```python
    config = GPTConfig(
        n_layer=36,
        n_head=20,
        n_embd=1280,
        vocab_size=50257,
        block_size=1024,
        bias=True,
        dropout=0.0
    )
    model = GPT.from_pretrained('gpt2-large')
    ```
    
    **Parameters:** 774M
  </Tab>
  
  <Tab title="GPT-2 XL (1558M)">
    ```python
    config = GPTConfig(
        n_layer=48,
        n_head=25,
        n_embd=1600,
        vocab_size=50257,
        block_size=1024,
        bias=True,
        dropout=0.0
    )
    model = GPT.from_pretrained('gpt2-xl')
    ```
    
    **Parameters:** 1558M
  </Tab>
</Tabs>

## Common Configurations

### Tiny Model (for testing)

```python
config = GPTConfig(
    block_size=256,
    vocab_size=50304,
    n_layer=4,
    n_head=4,
    n_embd=128,
    dropout=0.0,
    bias=False
)
```

**Use case:** Quick experiments, debugging, limited compute

### Small Model (~60M params)

```python
config = GPTConfig(
    block_size=512,
    vocab_size=50304,
    n_layer=6,
    n_head=6,
    n_embd=384,
    dropout=0.1,
    bias=False
)
```

**Use case:** Training on single GPU, medium datasets

### Custom Large Model

```python
config = GPTConfig(
    block_size=2048,      # Longer context
    vocab_size=50304,
    n_layer=32,
    n_head=16,
    n_embd=1024,
    dropout=0.0,
    bias=False
)
```

**Use case:** Large-scale training with multiple GPUs

## Creating Models

### From Scratch

```python
from model import GPT, GPTConfig

# Using defaults
config = GPTConfig()
model = GPT(config)

# Custom configuration
config = GPTConfig(
    n_layer=6,
    n_head=6,
    n_embd=384,
    block_size=256,
    dropout=0.1,
    bias=False
)
model = GPT(config)
```

### From Pretrained

```python
# Load pretrained GPT-2
model = GPT.from_pretrained('gpt2')
model = GPT.from_pretrained('gpt2-medium')
model = GPT.from_pretrained('gpt2-large')
model = GPT.from_pretrained('gpt2-xl')

# Override dropout for fine-tuning
model = GPT.from_pretrained('gpt2', override_args={'dropout': 0.1})
```

<Note>
Only the `dropout` parameter can be overridden when loading pretrained models. Other parameters are fixed by the checkpoint architecture.
</Note>

## Model Information

### Parameter Count

```python
model = GPT(config)

# Total parameters (including embeddings)
total_params = model.get_num_params(non_embedding=False)

# Non-embedding parameters (default)
trainable_params = model.get_num_params(non_embedding=True)

print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")
```

### Configuration Access

```python
# Access current configuration
print(model.config.n_layer)
print(model.config.n_embd)
print(model.config.block_size)

# Model automatically prints parameter count on initialization
# Output: "number of parameters: 124.44M"
```

## Optimizer Configuration

The model provides a helper method to configure optimizers with weight decay:

```python
optimizer = model.configure_optimizers(
    weight_decay=0.1,
    learning_rate=6e-4,
    betas=(0.9, 0.95),
    device_type='cuda'
)
```

**Features:**
- Automatically separates parameters into decay/no-decay groups
- 2D parameters (weights): apply weight decay
- 1D parameters (biases, layernorms): no weight decay
- Uses fused AdamW when available on CUDA

<Info>
The `configure_optimizers` method follows best practices by not applying weight decay to biases and layer normalization parameters.
</Info>

## See Also

- [Architecture](/model/architecture) - Model components and structure
- [Inference](/model/inference) - Text generation with configured models
