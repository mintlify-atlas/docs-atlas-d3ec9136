---
title: 'Model Architecture'
description: 'GPT model architecture and core components'
---

## Overview

The nanoGPT architecture is a clean, minimal implementation of the GPT (Generative Pre-trained Transformer) model. The entire model is defined in a single file (`model.py`) with approximately 300 lines of code.

<Info>
The implementation follows the original GPT-2 architecture from OpenAI with optimizations for training efficiency.
</Info>

## Architecture Components

The GPT model consists of five main components that work together to create a powerful language model:

### LayerNorm

A custom LayerNorm implementation with optional bias parameter.

```python
class LayerNorm(nn.Module):
    """ LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False """

    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None

    def forward(self, input):
        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)
```

**Key Features:**
- Optional bias parameter for efficiency
- Uses PyTorch's functional layer norm with epsilon=1e-5
- Applied before attention and MLP layers (pre-norm architecture)

### CausalSelfAttention

The attention mechanism that allows the model to focus on relevant parts of the input sequence.

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        # output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.dropout = config.dropout
        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
```

**Key Features:**
- **Multi-head attention**: Splits embedding dimension across multiple attention heads
- **Causal masking**: Ensures tokens can only attend to previous positions
- **Flash Attention support**: Automatically uses optimized CUDA kernels if PyTorch >= 2.0
- **Efficient projection**: Single linear layer for Q, K, V projections (3x n_embd)

<Note>
If Flash Attention is not available (PyTorch < 2.0), the implementation falls back to manual attention with a causal bias mask.
</Note>

**Attention Mechanism:**

```python
if self.flash:
    # efficient attention using Flash Attention CUDA kernels
    y = torch.nn.functional.scaled_dot_product_attention(
        q, k, v, attn_mask=None, 
        dropout_p=self.dropout if self.training else 0, 
        is_causal=True
    )
else:
    # manual implementation of attention
    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
    att = F.softmax(att, dim=-1)
    att = self.attn_dropout(att)
    y = att @ v
```

### MLP

The feed-forward neural network applied after attention in each transformer block.

```python
class MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        self.gelu    = nn.GELU()
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x
```

**Architecture:**
- Expansion layer: `n_embd → 4 * n_embd`
- GELU activation function
- Projection layer: `4 * n_embd → n_embd`
- Dropout for regularization

<Info>
The 4x expansion factor is standard in transformer architectures and provides the model with sufficient capacity for complex transformations.
</Info>

### Block

A single transformer block combining attention and MLP with residual connections.

```python
class Block(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
```

**Architecture Pattern:**
1. LayerNorm → Attention → Residual connection
2. LayerNorm → MLP → Residual connection

<Note>
This uses **pre-norm** architecture (LayerNorm before attention/MLP) which is more stable for training than post-norm.
</Note>

### GPT Model

The complete GPT model that stacks multiple transformer blocks.

```python
class GPT(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            wpe = nn.Embedding(config.block_size, config.n_embd),
            drop = nn.Dropout(config.dropout),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            ln_f = LayerNorm(config.n_embd, bias=config.bias),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        # Weight tying between token embeddings and output layer
        self.transformer.wte.weight = self.lm_head.weight
```

**Components:**
- `wte`: Token embeddings (vocab_size × n_embd)
- `wpe`: Position embeddings (block_size × n_embd)
- `drop`: Dropout layer
- `h`: Stack of transformer blocks
- `ln_f`: Final layer normalization
- `lm_head`: Output projection to vocabulary

<Info>
**Weight Tying**: The token embedding matrix is shared with the output projection layer (`lm_head`). This reduces parameters and often improves performance.
</Info>

## Forward Pass

The forward pass processes input tokens through the entire model:

```python
def forward(self, idx, targets=None):
    device = idx.device
    b, t = idx.size()
    assert t <= self.config.block_size
    pos = torch.arange(0, t, dtype=torch.long, device=device)

    # forward the GPT model itself
    tok_emb = self.transformer.wte(idx)  # token embeddings (b, t, n_embd)
    pos_emb = self.transformer.wpe(pos)  # position embeddings (t, n_embd)
    x = self.transformer.drop(tok_emb + pos_emb)
    for block in self.transformer.h:
        x = block(x)
    x = self.transformer.ln_f(x)

    if targets is not None:
        # training: calculate loss
        logits = self.lm_head(x)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), 
                               targets.view(-1), ignore_index=-1)
    else:
        # inference: only forward last position
        logits = self.lm_head(x[:, [-1], :])
        loss = None

    return logits, loss
```

**Steps:**
1. Create position indices
2. Embed tokens and positions, then sum
3. Apply dropout
4. Process through all transformer blocks
5. Apply final layer norm
6. Project to vocabulary (with optimization for inference)

<Note>
**Inference Optimization**: During inference (no targets), only the last position is projected through `lm_head`, saving computation.
</Note>

## Weight Initialization

The model uses careful weight initialization for stable training:

```python
def _init_weights(self, module):
    if isinstance(module, nn.Linear):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        if module.bias is not None:
            torch.nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

# Special scaled init for residual projections
for pn, p in self.named_parameters():
    if pn.endswith('c_proj.weight'):
        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))
```

**Initialization Strategy:**
- Linear and embedding layers: Normal(0, 0.02)
- Biases: Zero initialization
- Residual projections: Scaled by `1/sqrt(2*n_layer)` to account for residual path accumulation

## Loading Pretrained Models

The model supports loading pretrained GPT-2 checkpoints:

```python
model = GPT.from_pretrained('gpt2')         # 124M params
model = GPT.from_pretrained('gpt2-medium')  # 350M params
model = GPT.from_pretrained('gpt2-large')   # 774M params
model = GPT.from_pretrained('gpt2-xl')      # 1558M params
```

<Info>
Pretrained models are loaded from HuggingFace's transformers library and weights are automatically converted to nanoGPT's format.
</Info>

**Supported Models:**

| Model | Layers | Heads | Embedding Dim | Parameters |
|-------|--------|-------|---------------|------------|
| gpt2 | 12 | 12 | 768 | 124M |
| gpt2-medium | 24 | 16 | 1024 | 350M |
| gpt2-large | 36 | 20 | 1280 | 774M |
| gpt2-xl | 48 | 25 | 1600 | 1558M |

## Model Surgery

You can adjust the block size after initialization:

```python
def crop_block_size(self, block_size):
    assert block_size <= self.config.block_size
    self.config.block_size = block_size
    self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])
    for block in self.transformer.h:
        if hasattr(block.attn, 'bias'):
            block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]
```

This is useful when loading a pretrained model (block_size=1024) but wanting to use a smaller context window.

## Performance Utilities

### Parameter Counting

```python
model.get_num_params(non_embedding=True)
```

Counts model parameters, optionally excluding position embeddings (which aren't used during inference on longer sequences).

### Model FLOPs Utilization (MFU)

```python
mfu = model.estimate_mfu(fwdbwd_per_iter, dt)
```

Estimates the percentage of A100 GPU peak FLOPs being utilized, useful for measuring training efficiency.

## See Also

- [Configuration](/model/configuration) - GPTConfig and model parameters
- [Inference](/model/inference) - Text generation and sampling
