---
title: Introduction
description: The simplest, fastest repository for training/finetuning medium-sized GPTs
---

<Note>
  **Update Nov 2025:** nanoGPT has a new and improved cousin called [nanochat](https://github.com/karpathy/nanochat). It is very likely you meant to use/find nanochat instead. nanoGPT (this repo) is now very old and deprecated but I will leave it up for posterity.
</Note>

## Overview

The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of [minGPT](https://github.com/karpathy/minGPT) that prioritizes teeth over education. 

The code itself is remarkably concise and readable:
- `train.py` - **~300 lines** of boilerplate training loop
- `model.py` - **~300 lines** GPT model definition

Despite its simplicity, nanoGPT reproduces **GPT-2 (124M)** on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training.

## What Makes nanoGPT Special

<CardGroup cols={2}>
  <Card title="Simple & Readable" icon="code">
    Plain Python code that's easy to understand and hack. No complicated abstractions or frameworks.
  </Card>
  <Card title="Fast & Efficient" icon="bolt">
    Reproduces GPT-2 124M results. Supports PyTorch 2.0 compile for significant speedups.
  </Card>
  <Card title="Highly Hackable" icon="wrench">
    Easy to modify for your needs. Train new models from scratch or finetune pretrained checkpoints.
  </Card>
  <Card title="Production Ready" icon="rocket">
    Supports distributed training with DDP, multinode setups, and can optionally load GPT-2 weights from OpenAI.
  </Card>
</CardGroup>

## Key Features

- **Minimal dependencies** - Just PyTorch, NumPy, and a few utilities
- **Character-level and BPE tokenization** - Start simple or use OpenAI's fast BPE tokenizer
- **Multiple training modes** - Train from scratch, finetune pretrained models, or evaluate baselines
- **Cross-platform support** - Works on GPU (CUDA), CPU, and Apple Silicon (MPS)
- **Distributed training** - Built-in support for PyTorch DDP and multi-node training
- **Optional logging** - Integration with Weights & Biases for experiment tracking

## Get Started

<CardGroup cols={3}>
  <Card title="Quickstart" icon="play" href="/quickstart">
    Train your first character-level GPT on Shakespeare in 3 minutes
  </Card>
  <Card title="Installation" icon="download" href="/installation">
    Set up your environment and install dependencies
  </Card>
  <Card title="Training Guide" icon="graduation-cap" href="/training/overview">
    Learn how to train and finetune GPT models
  </Card>
</CardGroup>

## Community & Support

For more context on this repository, GPT, and language modeling, watch Andrej Karpathy's [Zero To Hero series](https://karpathy.ai/zero-to-hero.html). The [GPT video](https://www.youtube.com/watch?v=kCc8FmEb1nY) is particularly helpful if you have prior language modeling context.

Join the discussion on Discord: [#nanoGPT](https://discord.gg/3zy8kqD9Cp)
