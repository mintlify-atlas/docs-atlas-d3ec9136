{
  "projectType": "ai-ml",
  "projectName": "nanoGPT",
  "projectDescription": "The simplest, fastest repository for training/finetuning medium-sized GPTs.",
  "theme": "aspen",
  "primaryColor": "#10B981",
  "lightColor": "#34D399",
  "darkColor": "#059669",
  "navigation": {
    "tabs": [
      {
        "tab": "Documentation",
        "groups": [
          {
            "group": "Getting Started",
            "pages": [
              "introduction",
              "installation",
              "quickstart"
            ]
          },
          {
            "group": "Training",
            "pages": [
              "training/overview",
              "training/from-scratch",
              "training/distributed",
              "training/configuration"
            ]
          },
          {
            "group": "Finetuning",
            "pages": [
              "finetuning/overview",
              "finetuning/pretrained-models",
              "finetuning/custom-datasets"
            ]
          },
          {
            "group": "Guides",
            "pages": [
              "guides/data-preparation",
              "guides/sampling",
              "guides/benchmarking",
              "guides/optimization"
            ]
          }
        ]
      },
      {
        "tab": "API Reference",
        "groups": [
          {
            "group": "Core Modules",
            "pages": [
              "api/model",
              "api/training",
              "api/sampling",
              "api/configuration"
            ]
          },
          {
            "group": "Model Architecture",
            "pages": [
              "api/gpt-config",
              "api/gpt-model",
              "api/attention",
              "api/blocks"
            ]
          }
        ]
      },
      {
        "tab": "Examples",
        "groups": [
          {
            "group": "Use Cases",
            "pages": [
              "examples/shakespeare",
              "examples/gpt2-reproduction",
              "examples/single-gpu",
              "examples/multi-node"
            ]
          }
        ]
      }
    ]
  },
  "keyFeatures": [
    "Train GPT-2 124M in ~4 days on 8xA100",
    "Simple codebase: ~300 lines for training, ~300 lines for model",
    "Character-level and BPE tokenization support",
    "Distributed training with PyTorch DDP",
    "Load and finetune OpenAI GPT-2 checkpoints",
    "PyTorch 2.0 compile support for speed",
    "Flash Attention for efficient training",
    "Flexible configuration system"
  ],
  "publicApiSurface": [
    "GPT",
    "GPTConfig",
    "CausalSelfAttention",
    "Block",
    "LayerNorm",
    "MLP",
    "GPT.from_pretrained()",
    "GPT.generate()",
    "GPT.configure_optimizers()",
    "GPT.estimate_mfu()",
    "train.py",
    "sample.py",
    "bench.py"
  ]
}
