---
title: Quickstart
description: Train a character-level GPT on Shakespeare in minutes
---

<Note>
  If you're not a deep learning professional and you just want to feel the magic and get your feet wet, this is the fastest way to get started.
</Note>

## Train on Shakespeare

We'll train a character-level GPT on the complete works of Shakespeare. This is a perfect starting point to understand how nanoGPT works.

<Steps>
  <Step title="Download and Prepare the Data">
    Download Shakespeare's works as a single 1MB file and convert it into a stream of integers:
    
    ```bash
    python data/shakespeare_char/prepare.py
    ```
    
    This creates `train.bin` and `val.bin` in the `data/shakespeare_char/` directory containing the tokenized text.
  </Step>
  
  <Step title="Train Your Model">
    The training approach depends on your available hardware:
    
    <CodeGroup>
    ```bash GPU Training
    python train.py config/train_shakespeare_char.py
    ```
    
    ```bash CPU/Macbook Training
    python train.py config/train_shakespeare_char.py \
      --device=cpu \
      --compile=False \
      --eval_iters=20 \
      --log_interval=1 \
      --block_size=64 \
      --batch_size=12 \
      --n_layer=4 \
      --n_head=4 \
      --n_embd=128 \
      --max_iters=2000 \
      --lr_decay_iters=2000 \
      --dropout=0.0
    ```
    
    ```bash Apple Silicon (M1/M2/M3)
    python train.py config/train_shakespeare_char.py --device=mps
    ```
    </CodeGroup>
  </Step>
  
  <Step title="Generate Samples">
    Once training completes, generate text from your trained model:
    
    <CodeGroup>
    ```bash GPU
    python sample.py --out_dir=out-shakespeare-char
    ```
    
    ```bash CPU
    python sample.py --out_dir=out-shakespeare-char --device=cpu
    ```
    
    ```bash Apple Silicon
    python sample.py --out_dir=out-shakespeare-char --device=mps
    ```
    </CodeGroup>
  </Step>
</Steps>

## GPU Training Details

If you have an NVIDIA GPU, you can train a baby GPT with these specifications:

- **Context size:** 256 characters
- **Feature channels:** 384
- **Architecture:** 6-layer Transformer with 6 heads per layer
- **Training time:** ~3 minutes on an A100 GPU
- **Best validation loss:** 1.4697

The configuration is defined in `config/train_shakespeare_char.py`. Model checkpoints are saved to the `out-shakespeare-char` directory.

### Sample Output (GPU)

After just 3 minutes of training, the model generates surprisingly coherent Shakespearean text:

```
ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang'd
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
```

Not bad for a character-level model after 3 minutes! Better results are obtainable by finetuning a pretrained GPT-2 model on this dataset.

## CPU/Macbook Training Details

<Warning>
  CPU training uses a much smaller model and achieves lower quality results, but it's great for learning and experimentation.
</Warning>

If you don't have a GPU, you can still train a GPT by using smaller model parameters:

**Key differences from GPU training:**
- `--device=cpu` - Run on CPU instead of GPU
- `--compile=False` - Disable PyTorch 2.0 compile (not supported on all CPUs)
- `--eval_iters=20` - Faster but noisier evaluation (down from 200)
- `--block_size=64` - Smaller context (256 on GPU)
- `--batch_size=12` - Smaller batches (64 on GPU)
- `--n_layer=4 --n_head=4 --n_embd=128` - Much smaller model
- `--max_iters=2000` - Fewer training iterations
- `--dropout=0.0` - Less regularization since model is small

**Training time:** ~3 minutes on a modern CPU  
**Best validation loss:** 1.88 (vs 1.47 on GPU)

### Sample Output (CPU)

The quality is lower but still shows the model learned Shakespearean patterns:

```
GLEORKEN VINGHARD III:
Whell's the couse, the came light gacks,
And the for mought you in Aut fries the not high shee
bot thou the sought bechive in that to doth groan you,
No relving thee post mose the wear
```

<Tip>
  If you're willing to wait longer, tune the hyperparameters to increase model size, context length (`--block_size`), and training duration (`--max_iters`).
</Tip>

## Apple Silicon Optimization

On Apple Silicon Macbooks (M1/M2/M3) with recent PyTorch versions, use the Metal Performance Shaders backend:

```bash
python train.py config/train_shakespeare_char.py --device=mps
```

The on-chip GPU can **significantly accelerate training (2-3X speedup)** and allow you to use larger networks compared to CPU training.

See [Issue #28](https://github.com/karpathy/nanoGPT/issues/28) for more details and discussion.

## Understanding the Training Configuration

The training behavior is controlled by the config file and command-line arguments:

<AccordionGroup>
  <Accordion title="Model Architecture Parameters">
    - `--n_layer` - Number of transformer layers (depth)
    - `--n_head` - Number of attention heads per layer
    - `--n_embd` - Embedding dimension size
    - `--block_size` - Context length (max sequence length)
    - `--dropout` - Dropout rate for regularization
  </Accordion>
  
  <Accordion title="Training Parameters">
    - `--batch_size` - Number of sequences per training step
    - `--max_iters` - Total number of training iterations
    - `--lr_decay_iters` - When to finish learning rate decay
    - `--eval_iters` - Number of iterations for validation evaluation
    - `--log_interval` - How often to log training metrics
  </Accordion>
  
  <Accordion title="System Parameters">
    - `--device` - Device to use: `cuda`, `cpu`, or `mps`
    - `--compile` - Enable PyTorch 2.0 compile for speedup
    - `--out_dir` - Directory to save checkpoints
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Reproduce GPT-2" icon="robot" href="/training/reproduce-gpt2">
    Train GPT-2 124M from scratch on OpenWebText
  </Card>
  <Card title="Finetune Pretrained Models" icon="arrows-rotate" href="/training/finetuning">
    Start from GPT-2 checkpoints and finetune on your data
  </Card>
  <Card title="Sampling and Inference" icon="wand-magic-sparkles" href="/inference/sampling">
    Learn how to generate text from your trained models
  </Card>
  <Card title="Distributed Training" icon="network-wired" href="/training/distributed">
    Scale up with multi-GPU and multi-node training
  </Card>
</CardGroup>

## Tips and Tricks

<Tip>
  **Starting from pretrained GPT-2:** Instead of training a character-level model from scratch, you can finetune GPT-2 on Shakespeare for much better results. See the [finetuning guide](/training/finetuning) for details.
</Tip>

<Tip>
  **Custom prompts:** You can prompt the model with text from a file:  
  ```bash
  python sample.py --start=FILE:prompt.txt
  ```
</Tip>

<Tip>
  **Experiment with hyperparameters:** The provided configs are starting points. Feel free to adjust model size, learning rates, and training duration based on your hardware and quality requirements.
</Tip>
