---
title: Installation
description: Set up your environment and install nanoGPT dependencies
---

## Quick Install

Install all dependencies with a single pip command:

```bash
pip install torch numpy transformers datasets tiktoken wandb tqdm
```

## Dependencies

Here's what each package does:

<AccordionGroup>
  <Accordion title="pytorch - Core deep learning framework">
    The foundation of nanoGPT. Handles all tensor operations, autograd, and GPU acceleration.
    
    [Visit PyTorch.org](https://pytorch.org)
  </Accordion>
  
  <Accordion title="numpy - Numerical computing">
    Used for efficient array operations and data preprocessing.
    
    [Visit numpy.org](https://numpy.org/install/)
  </Accordion>
  
  <Accordion title="transformers - Hugging Face library">
    Provides access to pretrained GPT-2 checkpoints for finetuning.
    
    Used when you want to start from OpenAI's pretrained models.
  </Accordion>
  
  <Accordion title="datasets - Hugging Face datasets">
    Simplifies downloading and preprocessing datasets like OpenWebText.
    
    Required if you want to reproduce GPT-2 training on OpenWebText.
  </Accordion>
  
  <Accordion title="tiktoken - OpenAI's BPE tokenizer">
    Fast Byte Pair Encoding tokenizer used by GPT-2 and GPT-3.
    
    Essential for reproducing GPT-2 results with proper tokenization.
  </Accordion>
  
  <Accordion title="wandb - Weights & Biases logging">
    Optional experiment tracking and visualization.
    
    Track training metrics, visualize loss curves, and compare runs.
  </Accordion>
  
  <Accordion title="tqdm - Progress bars">
    Provides visual progress indicators during training and data preparation.
  </Accordion>
</AccordionGroup>

## Platform-Specific Setup

<Steps>
  <Step title="Choose Your Platform">
    nanoGPT works on multiple platforms. Select the appropriate setup for your hardware:
    
    - **CUDA GPU** (NVIDIA) - Best performance
    - **Apple Silicon** (M1/M2/M3) - Good performance with MPS
    - **CPU** - Slowest but works everywhere
  </Step>
  
  <Step title="Install PyTorch">
    Visit [PyTorch Get Started](https://pytorch.org/get-started/locally/) and select your platform.
    
    <CodeGroup>
    ```bash CUDA (Linux/Windows)
    # For CUDA 11.8
    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
    
    # For CUDA 12.1
    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    ```
    
    ```bash CPU Only
    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
    ```
    
    ```bash Mac (Apple Silicon)
    # MPS (Metal Performance Shaders) support built-in
    pip3 install torch torchvision torchaudio
    ```
    </CodeGroup>
  </Step>
  
  <Step title="Install Remaining Dependencies">
    After PyTorch is installed, add the other dependencies:
    
    ```bash
    pip install numpy transformers datasets tiktoken wandb tqdm
    ```
  </Step>
  
  <Step title="Verify Installation">
    Test that PyTorch is properly installed and can detect your hardware:
    
    ```python
    import torch
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    print(f"MPS available: {torch.backends.mps.is_available()}")
    ```
  </Step>
</Steps>

## PyTorch 2.0 and torch.compile

<Note>
  nanoGPT uses PyTorch 2.0's `torch.compile()` by default for significant performance improvements.
</Note>

PyTorch 2.0 introduced `torch.compile()` which can dramatically speed up training:
- **Before:** ~250ms per iteration
- **After:** ~135ms per iteration

This feature is included in PyTorch 2.0+ and works automatically. For cutting-edge performance, consider using the PyTorch nightly build:

```bash
pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu118
```

## GPU Setup Notes

### NVIDIA CUDA

For optimal performance on NVIDIA GPUs:

1. Install the latest NVIDIA drivers
2. Install CUDA Toolkit matching your PyTorch version
3. Verify CUDA is working:
   ```bash
   python -c "import torch; print(torch.cuda.is_available())"
   ```

### Apple Silicon (M1/M2/M3)

<Warning>
  Make sure you have a recent PyTorch version to use MPS acceleration on Apple Silicon.
</Warning>

On Apple Silicon Macbooks, use the `--device=mps` flag (Metal Performance Shaders) to leverage the on-chip GPU:

```bash
python train.py config/train_shakespeare_char.py --device=mps
```

This can provide **2-3x speedup** compared to CPU training. See [Issue #28](https://github.com/karpathy/nanoGPT/issues/28) for more details.

## Troubleshooting

<AccordionGroup>
  <Accordion title="torch.compile() errors on Windows">
    PyTorch 2.0's `torch.compile()` is not yet available on all platforms (especially Windows). If you encounter errors:
    
    ```bash
    python train.py --compile=False
    ```
    
    This will slow down training but ensures compatibility.
  </Accordion>
  
  <Accordion title="Out of memory errors">
    If you run out of GPU/CPU memory:
    
    1. Decrease batch size: `--batch_size=8`
    2. Reduce model size: Use smaller `--n_layer`, `--n_head`, `--n_embd`
    3. Decrease context length: `--block_size=128`
  </Accordion>
  
  <Accordion title="Slow training on CPU">
    CPU training is significantly slower than GPU. To speed it up:
    
    1. Use smaller models (fewer layers and smaller embedding dimensions)
    2. Reduce batch size and context length
    3. Decrease `--max_iters` for quick experiments
    4. Consider using `--compile=False` if compile overhead is significant
  </Accordion>
  
  <Accordion title="CUDA version mismatch">
    Ensure your PyTorch CUDA version matches your installed CUDA toolkit:
    
    ```python
    import torch
    print(torch.version.cuda)  # PyTorch's CUDA version
    ```
    
    If there's a mismatch, reinstall PyTorch with the correct CUDA version.
  </Accordion>
</AccordionGroup>

## Next Steps

Now that you have nanoGPT installed, try the [Quickstart guide](/quickstart) to train your first model on Shakespeare!
