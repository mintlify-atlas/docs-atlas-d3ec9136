---
title: 'Available Datasets'
description: 'Explore the datasets included with nanoGPT'
---

# Available Datasets

NanoGPT includes several pre-configured datasets for quick experimentation and GPT-2 reproduction.

## Dataset Overview

| Dataset | Size | Tokens | Tokenization | Use Case |
|---------|------|--------|--------------|----------|
| shakespeare_char | 1.1M chars | 1M train | Character-level | Quick experiments |
| shakespeare | 1.1M chars | 300K train | GPT-2 BPE | Small-scale testing |
| openwebtext | 8M docs | 9B train | GPT-2 BPE | GPT-2 reproduction |

## Shakespeare (Character-Level)

### Overview

The Tiny Shakespeare dataset with character-level tokenization. Perfect for fast experimentation and debugging.

**Location**: `data/shakespeare_char/`

### Statistics

- **Source**: 1,115,394 characters from Shakespeare's works
- **Vocabulary**: 65 unique characters
- **Training tokens**: 1,003,854
- **Validation tokens**: 111,540
- **Train/val split**: 90/10

### Characters in Vocabulary

```
 !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
```

### Preparation

<Steps>
  <Step title="Navigate to directory">
    ```bash
    cd data/shakespeare_char
    ```
  </Step>
  
  <Step title="Run preparation script">
    ```bash
    python prepare.py
    ```
    This downloads the dataset and creates `train.bin`, `val.bin`, and `meta.pkl`
  </Step>
  
  <Step title="Train a model">
    ```bash
    cd ../..
    python train.py config/train_shakespeare_char.py
    ```
  </Step>
</Steps>

### Preparation Script

```python
import os
import pickle
import requests
import numpy as np

# Download the tiny shakespeare dataset
input_file_path = os.path.join(os.path.dirname(__file__), 'input.txt')
if not os.path.exists(input_file_path):
    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'
    with open(input_file_path, 'w') as f:
        f.write(requests.get(data_url).text)

with open(input_file_path, 'r') as f:
    data = f.read()

# Get all unique characters
chars = sorted(list(set(data)))
vocab_size = len(chars)

# Create character mappings
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}

def encode(s):
    return [stoi[c] for c in s]

# Create train/val split
n = len(data)
train_data = data[:int(n*0.9)]
val_data = data[int(n*0.9):]

# Encode to integers
train_ids = encode(train_data)
val_ids = encode(val_data)

# Export to bin files
train_ids = np.array(train_ids, dtype=np.uint16)
val_ids = np.array(val_ids, dtype=np.uint16)
train_ids.tofile(os.path.join(os.path.dirname(__file__), 'train.bin'))
val_ids.tofile(os.path.join(os.path.dirname(__file__), 'val.bin'))

# Save metadata
meta = {
    'vocab_size': vocab_size,
    'itos': itos,
    'stoi': stoi,
}
with open(os.path.join(os.path.dirname(__file__), 'meta.pkl'), 'wb') as f:
    pickle.dump(meta, f)
```

<Note>
Character-level tokenization creates much longer sequences than BPE. The same text produces ~3x more tokens with character-level encoding.
</Note>

## Shakespeare (BPE)

### Overview

The Tiny Shakespeare dataset with GPT-2 BPE tokenization. Good for testing BPE tokenization with a small dataset.

**Location**: `data/shakespeare/`

### Statistics

- **Source**: 1,115,394 characters from Shakespeare's works
- **Vocabulary**: GPT-2 vocabulary (50,257 tokens)
- **Training tokens**: 301,966
- **Validation tokens**: 36,059
- **Train/val split**: 90/10

### Preparation

<Steps>
  <Step title="Navigate to directory">
    ```bash
    cd data/shakespeare
    ```
  </Step>
  
  <Step title="Run preparation script">
    ```bash
    python prepare.py
    ```
    This downloads the dataset and creates `train.bin` and `val.bin`
  </Step>
  
  <Step title="Train a model">
    ```bash
    cd ../..
    python train.py config/train_shakespeare.py
    ```
  </Step>
</Steps>

### Preparation Script

```python
import os
import requests
import tiktoken
import numpy as np

# Download the tiny shakespeare dataset
input_file_path = os.path.join(os.path.dirname(__file__), 'input.txt')
if not os.path.exists(input_file_path):
    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'
    with open(input_file_path, 'w', encoding='utf-8') as f:
        f.write(requests.get(data_url).text)

with open(input_file_path, 'r', encoding='utf-8') as f:
    data = f.read()

# Create train/val split
n = len(data)
train_data = data[:int(n*0.9)]
val_data = data[int(n*0.9):]

# Encode with tiktoken GPT-2 BPE
enc = tiktoken.get_encoding("gpt2")
train_ids = enc.encode_ordinary(train_data)
val_ids = enc.encode_ordinary(val_data)

print(f"train has {len(train_ids):,} tokens")
print(f"val has {len(val_ids):,} tokens")

# Export to bin files
train_ids = np.array(train_ids, dtype=np.uint16)
val_ids = np.array(val_ids, dtype=np.uint16)
train_ids.tofile(os.path.join(os.path.dirname(__file__), 'train.bin'))
val_ids.tofile(os.path.join(os.path.dirname(__file__), 'val.bin'))
```

<Note>
BPE tokenization produces ~3x fewer tokens than character-level for the same text, making training more efficient.
</Note>

## OpenWebText

### Overview

An open-source recreation of OpenAI's WebText dataset used to train GPT-2. This is a large-scale dataset for reproducing GPT-2's performance.

**Location**: `data/openwebtext/`

### Statistics

- **Source**: 8,013,769 documents from Reddit submissions
- **Vocabulary**: GPT-2 vocabulary (50,257 tokens)
- **Training tokens**: ~9 billion (9,035,582,198)
- **Validation tokens**: ~4 million (4,434,897)
- **Train/val split**: 99.95%/0.05%
- **File sizes**: train.bin ~17GB, val.bin ~8.5MB

### Preparation

<Warning>
Preparing OpenWebText requires:
- ~54GB disk space for the Hugging Face cache
- ~17GB for the final train.bin file
- Several hours of processing time
- Good internet connection for downloading
</Warning>

<Steps>
  <Step title="Navigate to directory">
    ```bash
    cd data/openwebtext
    ```
  </Step>
  
  <Step title="Run preparation script">
    ```bash
    python prepare.py
    ```
    This downloads ~54GB of data and processes it into binary format
  </Step>
  
  <Step title="Train GPT-2">
    ```bash
    cd ../..
    python train.py config/train_gpt2.py
    ```
  </Step>
</Steps>

### Preparation Script Details

The OpenWebText preparation script uses advanced techniques for processing large datasets:

```python
import os
from tqdm import tqdm
import numpy as np
import tiktoken
from datasets import load_dataset

num_proc = 8  # Number of parallel workers

enc = tiktoken.get_encoding("gpt2")

if __name__ == '__main__':
    # Load dataset from Hugging Face
    dataset = load_dataset("openwebtext", num_proc=num_proc)
    
    # Create train/val split
    split_dataset = dataset["train"].train_test_split(
        test_size=0.0005, seed=2357, shuffle=True
    )
    split_dataset['val'] = split_dataset.pop('test')
    
    # Tokenization function
    def process(example):
        ids = enc.encode_ordinary(example['text'])
        ids.append(enc.eot_token)  # Add end-of-text token (50256)
        out = {'ids': ids, 'len': len(ids)}
        return out
    
    # Tokenize in parallel
    tokenized = split_dataset.map(
        process,
        remove_columns=['text'],
        desc="tokenizing the splits",
        num_proc=num_proc,
    )
    
    # Write to memory-mapped binary files
    for split, dset in tokenized.items():
        arr_len = np.sum(dset['len'], dtype=np.uint64)
        filename = os.path.join(os.path.dirname(__file__), f'{split}.bin')
        dtype = np.uint16
        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))
        total_batches = 1024
        
        idx = 0
        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):
            # Batch together samples for faster write
            batch = dset.shard(
                num_shards=total_batches, 
                index=batch_idx, 
                contiguous=True
            ).with_format('numpy')
            arr_batch = np.concatenate(batch['ids'])
            arr[idx : idx + len(arr_batch)] = arr_batch
            idx += len(arr_batch)
        arr.flush()
```

### Key Features

<CodeGroup>
```python Parallel Processing
# Use multiple CPU cores for faster tokenization
num_proc = 8
tokenized = split_dataset.map(
    process,
    num_proc=num_proc,
)
```

```python Memory-Mapped Writing
# Write directly to disk without loading entire dataset into RAM
arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))
```

```python Batched Writing
# Write in batches for better performance
total_batches = 1024
for batch_idx in range(total_batches):
    batch = dset.shard(num_shards=total_batches, index=batch_idx)
    arr[idx : idx + len(arr_batch)] = arr_batch
```
</CodeGroup>

### End-of-Text Tokens

Each document in OpenWebText is separated by an end-of-text token (ID: 50256):

```python
ids = enc.encode_ordinary(example['text'])
ids.append(enc.eot_token)  # 50256 for GPT-2
```

<Note>
The EOT token helps the model learn document boundaries, which is important for coherent text generation.
</Note>

## Dataset Selection Guide

### For Quick Experiments

Use `shakespeare_char`:
- Fastest to prepare (&lt;1 second)
- Smallest dataset (~1M tokens)
- Good for testing changes quickly
- Character-level tokenization

### For Testing BPE Tokenization

Use `shakespeare`:
- Fast to prepare (&lt;1 second)
- Small dataset (~300K tokens)
- Uses GPT-2 BPE tokenization
- Good for validating BPE pipelines

### For GPT-2 Reproduction

Use `openwebtext`:
- Large-scale dataset (9B tokens)
- Same dataset family as GPT-2
- Requires significant compute
- Full BPE tokenization

## Reading Binary Files

To inspect or use the prepared binary files:

```python
import numpy as np

# Load as memory-mapped array (doesn't load into RAM)
data = np.memmap('train.bin', dtype=np.uint16, mode='r')

print(f"Dataset has {len(data):,} tokens")
print(f"First 10 token IDs: {data[:10]}")

# For character-level datasets, decode using meta.pkl
import pickle
with open('meta.pkl', 'rb') as f:
    meta = pickle.load(f)

itos = meta['itos']
decoded = ''.join([itos[int(i)] for i in data[:100]])
print(f"First 100 characters: {decoded}")
```

## Next Steps

- Learn about [data preparation](/data/preparation)
- Understand [tokenization approaches](/data/tokenization)
- Configure [training parameters](/training/overview)
