---
title: 'Tokenization'
description: 'Understanding character-level and BPE tokenization in nanoGPT'
---

# Tokenization

NanoGPT supports two tokenization approaches: character-level and Byte Pair Encoding (BPE). Each has different trade-offs for vocabulary size, sequence length, and model performance.

## Tokenization Overview

| Approach | Vocabulary Size | Sequence Length | Use Case |
|----------|----------------|-----------------|----------|
| Character-level | Small (65-256) | Long (3x more tokens) | Simple experiments, small datasets |
| BPE (tiktoken) | Large (50,257) | Short (3x fewer tokens) | Production, GPT-2 reproduction |

## Character-Level Tokenization

### Overview

Character-level tokenization maps each unique character to an integer ID. This is the simplest form of tokenization.

**Example**: `"hello"` → `[46, 43, 50, 50, 53]`

### Vocabulary

The vocabulary is built from all unique characters in your dataset:

```python
# From shakespeare_char/prepare.py:24-27
chars = sorted(list(set(data)))
vocab_size = len(chars)
print("all the unique characters:", ''.join(chars))
print(f"vocab size: {vocab_size:,}")
```

For Tiny Shakespeare:
```
all the unique characters: !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
vocab size: 65
```

### Encoding and Decoding

Create bidirectional mappings between characters and integers:

```python
# From shakespeare_char/prepare.py:29-35
stoi = {ch: i for i, ch in enumerate(chars)}  # string to int
itos = {i: ch for i, ch in enumerate(chars)}  # int to string

def encode(s):
    return [stoi[c] for c in s]  # string -> list of ints

def decode(l):
    return ''.join([itos[i] for i in l])  # list of ints -> string
```

### Complete Implementation

Here's the full character-level tokenization pipeline:

```python
import os
import pickle
import numpy as np

# Load text data
with open('input.txt', 'r') as f:
    data = f.read()
print(f"length of dataset in characters: {len(data):,}")

# Build vocabulary from unique characters
chars = sorted(list(set(data)))
vocab_size = len(chars)
print(f"vocab size: {vocab_size:,}")

# Create character <-> integer mappings
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}

def encode(s):
    return [stoi[c] for c in s]

def decode(l):
    return ''.join([itos[i] for i in l])

# Create train/val split
n = len(data)
train_data = data[:int(n*0.9)]
val_data = data[int(n*0.9):]

# Encode both splits
train_ids = encode(train_data)
val_ids = encode(val_data)
print(f"train has {len(train_ids):,} tokens")
print(f"val has {len(val_ids):,} tokens")

# Export to binary files
train_ids = np.array(train_ids, dtype=np.uint16)
val_ids = np.array(val_ids, dtype=np.uint16)
train_ids.tofile('train.bin')
val_ids.tofile('val.bin')

# Save metadata for later use
meta = {
    'vocab_size': vocab_size,
    'itos': itos,
    'stoi': stoi,
}
with open('meta.pkl', 'wb') as f:
    pickle.dump(meta, f)
```

### Metadata Storage

Character-level tokenization requires storing the vocabulary mappings in `meta.pkl`:

```python
# Saved during preparation
meta = {
    'vocab_size': 65,
    'itos': {0: '\n', 1: ' ', 2: '!', ...},
    'stoi': {'\n': 0, ' ': 1, '!': 2, ...},
}
```

This metadata is loaded during training to initialize the model:

```python
# From train.py:137-144
meta_path = os.path.join(data_dir, 'meta.pkl')
meta_vocab_size = None
if os.path.exists(meta_path):
    with open(meta_path, 'rb') as f:
        meta = pickle.load(f)
    meta_vocab_size = meta['vocab_size']
    print(f"found vocab_size = {meta_vocab_size} (inside {meta_path})")
```

### Advantages

<CardGroup cols={2}>
  <Card title="Simple" icon="sparkles">
    No external dependencies or pre-trained tokenizers needed
  </Card>
  
  <Card title="Small Vocabulary" icon="book">
    Typically 65-256 tokens, reducing embedding layer size
  </Card>
  
  <Card title="No UNK Tokens" icon="check">
    Can represent any character in the training set
  </Card>
  
  <Card title="Fast Preparation" icon="bolt">
    Tokenization is instant (just character lookup)
  </Card>
</CardGroup>

### Disadvantages

<Warning>
Character-level tokenization creates much longer sequences:
- Same text produces ~3x more tokens than BPE
- Requires more compute for same amount of text
- Model needs to learn character-to-word composition
- Less efficient for large-scale training
</Warning>

### Example: Shakespeare Statistics

```
Dataset: Tiny Shakespeare (1,115,394 characters)

Character-level tokenization:
- Vocab size: 65
- Train tokens: 1,003,854
- Val tokens: 111,540
- Tokens per character: 1.0

BPE tokenization (for comparison):
- Vocab size: 50,257
- Train tokens: 301,966
- Val tokens: 36,059
- Tokens per character: 0.30
```

## BPE Tokenization (tiktoken)

### Overview

Byte Pair Encoding (BPE) learns a vocabulary of subword units, balancing between character-level and word-level tokenization. NanoGPT uses [tiktoken](https://github.com/openai/tiktoken), OpenAI's fast BPE tokenizer.

**Example**: `"hello"` → `[31373]` (single token)

### Using tiktoken

Tiktoken provides pre-trained BPE encodings including GPT-2's vocabulary:

```python
import tiktoken

# Get the GPT-2 BPE encoder
enc = tiktoken.get_encoding("gpt2")

# Encode text to token IDs
text = "Hello, world!"
token_ids = enc.encode_ordinary(text)
print(token_ids)  # [15496, 11, 995, 0]

# Decode token IDs back to text
decoded = enc.decode(token_ids)
print(decoded)  # "Hello, world!"
```

<Note>
`encode_ordinary()` is used instead of `encode()` because it ignores special tokens. This is appropriate for training data that doesn't contain special control tokens.
</Note>

### GPT-2 Vocabulary

The GPT-2 BPE vocabulary has:
- **50,257 tokens**: Base vocabulary
- **50,256 (0xC350)**: End-of-text (EOT) token
- Includes individual bytes, common words, and subword units

```python
enc = tiktoken.get_encoding("gpt2")
print(f"Vocabulary size: {enc.n_vocab}")  # 50257
print(f"Max token value: {enc.max_token_value}")  # 50256
print(f"EOT token: {enc.eot_token}")  # 50256
```

### Complete Implementation

Here's the full BPE tokenization pipeline:

```python
import os
import tiktoken
import numpy as np

# Load text data
with open('input.txt', 'r', encoding='utf-8') as f:
    data = f.read()

# Create train/val split
n = len(data)
train_data = data[:int(n*0.9)]
val_data = data[int(n*0.9):]

# Encode with GPT-2 BPE
enc = tiktoken.get_encoding("gpt2")
train_ids = enc.encode_ordinary(train_data)
val_ids = enc.encode_ordinary(val_data)

print(f"train has {len(train_ids):,} tokens")
print(f"val has {len(val_ids):,} tokens")

# Export to binary files
train_ids = np.array(train_ids, dtype=np.uint16)
val_ids = np.array(val_ids, dtype=np.uint16)
train_ids.tofile('train.bin')
val_ids.tofile('val.bin')
```

<Note>
No `meta.pkl` file is needed for BPE tokenization because the vocabulary is standard (GPT-2's 50,257 tokens).
</Note>

### End-of-Text Tokens

For datasets with multiple documents (like OpenWebText), add EOT tokens between documents:

```python
# From openwebtext/prepare.py:43-48
def process(example):
    ids = enc.encode_ordinary(example['text'])
    ids.append(enc.eot_token)  # Add EOT token (50256)
    out = {'ids': ids, 'len': len(ids)}
    return out
```

This helps the model learn document boundaries:
```
[tokens from doc1...] [50256] [tokens from doc2...] [50256] ...
```

### uint16 Compatibility

GPT-2's vocabulary (50,257 tokens) fits perfectly in `uint16` (max value: 65,535):

```python
# From openwebtext/prepare.py:62
dtype = np.uint16  # Can do since enc.max_token_value == 50256 < 2**16
arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))
```

This provides 2x memory savings compared to `uint32`.

### Advantages

<CardGroup cols={2}>
  <Card title="Efficient Sequences" icon="compress">
    3x fewer tokens than character-level for same text
  </Card>
  
  <Card title="Subword Units" icon="puzzle-piece">
    Balances vocabulary size and sequence length
  </Card>
  
  <Card title="Standard Vocabulary" icon="globe">
    Compatible with GPT-2 and other models
  </Card>
  
  <Card title="Better Performance" icon="chart-line">
    Models learn faster with subword tokenization
  </Card>
</CardGroup>

### Disadvantages

- **Larger vocabulary**: 50,257 tokens vs 65 for character-level
- **External dependency**: Requires tiktoken library
- **Fixed vocabulary**: Can't easily adapt to new domains
- **UNK tokens**: Rare byte sequences may not tokenize optimally

## Tokenization Comparison

### Same Text, Different Tokenization

Let's compare how the same text is tokenized:

<CodeGroup>
```python Character-Level
text = "hello world"
chars = ['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']
token_ids = [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]
num_tokens = 11
vocab_size = 65
```

```python BPE (tiktoken)
text = "hello world"
tokens = ['hello', ' world']
token_ids = [31373, 995]
num_tokens = 2
vocab_size = 50257
```
</CodeGroup>

BPE produces 5.5x fewer tokens for this example!

### Shakespeare Dataset Comparison

| Metric | Character-Level | BPE | Ratio |
|--------|----------------|-----|-------|
| Vocabulary size | 65 | 50,257 | 774x more |
| Train tokens | 1,003,854 | 301,966 | 3.3x fewer |
| Val tokens | 111,540 | 36,059 | 3.1x fewer |
| File size (train.bin) | 2.0 MB | 0.6 MB | 3.3x smaller |
| Embedding parameters | 65 × 768 | 50,257 × 768 | 774x more |

### Performance Implications

<Steps>
  <Step title="Sequence Length">
    BPE creates 3x shorter sequences, reducing:
    - Attention computation (quadratic in sequence length)
    - Memory usage during training
    - Training time per batch
  </Step>
  
  <Step title="Vocabulary Size">
    Character-level has 774x smaller vocabulary, reducing:
    - Embedding layer parameters (65 vs 50,257)
    - Output layer parameters (same as embedding)
    - Memory for embedding tables
  </Step>
  
  <Step title="Overall Impact">
    For most use cases, BPE's shorter sequences provide larger benefits than character-level's smaller vocabulary
  </Step>
</Steps>

## Choosing a Tokenization Approach

### Use Character-Level When:

- Experimenting with small datasets
- Debugging model architectures quickly
- Working with non-standard character sets
- Vocabulary size is a major constraint
- Sequence length is not a bottleneck

### Use BPE When:

- Training production models
- Reproducing GPT-2 or similar models
- Working with large-scale datasets
- Sequence length is a constraint (e.g., limited context window)
- Training time is important
- You want better sample efficiency

## Advanced Topics

### Custom BPE Vocabularies

While nanoGPT uses the standard GPT-2 vocabulary, you can train custom BPE vocabularies:

```python
import tiktoken

# Note: Training custom vocabularies requires additional setup
# not included in nanoGPT. The standard approach is to use
# libraries like tokenizers (Hugging Face) or sentencepiece
```

<Warning>
Training custom BPE vocabularies is not covered by nanoGPT's default scripts. For custom tokenization, you'd need to integrate additional libraries.
</Warning>

### Vocabulary Size Trade-offs

| Vocabulary Size | Token Efficiency | Model Size | Training Speed |
|-----------------|------------------|------------|----------------|
| Small (65) | Low (1.0x) | Small | Slow (long sequences) |
| Medium (5K-10K) | Medium (2x) | Medium | Medium |
| Large (50K) | High (3x) | Large | Fast (short sequences) |
| Very Large (100K+) | Very High (4x+) | Very Large | Very Fast |

### Memory Considerations

```python
# Embedding layer memory usage
vocab_size = 50257
embed_dim = 768

# Embedding parameters
embedding_params = vocab_size * embed_dim  # 38,597,376

# At float32 (4 bytes per param)
embedding_memory_mb = embedding_params * 4 / 1024 / 1024  # ~147 MB

# Output layer (tied weights) - same size as embedding
total_memory_mb = embedding_memory_mb * 2  # ~294 MB
```

## Practical Examples

### Inspecting Tokenization

<CodeGroup>
```python Character-Level
import pickle

# Load vocabulary
with open('data/shakespeare_char/meta.pkl', 'rb') as f:
    meta = pickle.load(f)

stoi = meta['stoi']
itos = meta['itos']

# Encode
text = "ROMEO:"
tokens = [stoi[c] for c in text]
print(f"Tokens: {tokens}")
print(f"Num tokens: {len(tokens)}")

# Decode
decoded = ''.join([itos[i] for i in tokens])
print(f"Decoded: {decoded}")
```

```python BPE
import tiktoken

enc = tiktoken.get_encoding("gpt2")

# Encode
text = "ROMEO:"
tokens = enc.encode_ordinary(text)
print(f"Tokens: {tokens}")
print(f"Num tokens: {len(tokens)}")

# Decode
decoded = enc.decode(tokens)
print(f"Decoded: {decoded}")

# Inspect tokens
for token in tokens:
    print(f"{token}: {enc.decode([token])}")
```
</CodeGroup>

### Converting Between Formats

If you have character-level data and want to convert to BPE:

```python
import pickle
import tiktoken
import numpy as np

# Load character-level data
with open('data/shakespeare_char/meta.pkl', 'rb') as f:
    meta = pickle.load(f)

char_data = np.memmap('data/shakespeare_char/train.bin', 
                      dtype=np.uint16, mode='r')

# Decode to text
itos = meta['itos']
text = ''.join([itos[int(i)] for i in char_data])

# Re-encode with BPE
enc = tiktoken.get_encoding("gpt2")
bpe_ids = enc.encode_ordinary(text)

# Save as BPE format
bpe_ids = np.array(bpe_ids, dtype=np.uint16)
bpe_ids.tofile('train_bpe.bin')

print(f"Character tokens: {len(char_data):,}")
print(f"BPE tokens: {len(bpe_ids):,}")
print(f"Compression ratio: {len(char_data) / len(bpe_ids):.2f}x")
```

## Next Steps

- Explore [available datasets](/data/datasets)
- Learn about [data preparation](/data/preparation)
- Configure [model architecture](/model/architecture) for your vocabulary size
- Start [training](/training/overview) your model
