---
title: 'MLP'
description: 'Position-wise feed-forward network with GELU activation'
---

# MLP

The `MLP` class implements a two-layer feed-forward network that is applied position-wise to each token independently. It follows each attention layer in the transformer.

## Class Definition

```python
class MLP(nn.Module):
    def __init__(self, config: GPTConfig)
```

## Architecture

The MLP consists of:
1. **Expansion layer**: Projects from `n_embd` to `4 * n_embd`
2. **GELU activation**: Non-linear transformation
3. **Projection layer**: Projects back to `n_embd`
4. **Dropout**: Regularization

```python
class MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        self.gelu    = nn.GELU()
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x
```

## Initialization

<ParamField path="config" type="GPTConfig" required>
  Configuration object containing:
  - `n_embd`: Input/output embedding dimension
  - `dropout`: Dropout probability
  - `bias`: Whether to use bias in linear layers
</ParamField>

```python
from model import GPTConfig, MLP

config = GPTConfig(n_embd=768, dropout=0.1, bias=False)
mlp = MLP(config)
```

## Forward Pass

<ParamField path="x" type="torch.FloatTensor" required>
  Input tensor of shape `(batch_size, sequence_length, n_embd)`
</ParamField>

<ResponseField name="output" type="torch.FloatTensor">
  Output tensor of shape `(batch_size, sequence_length, n_embd)` (same as input)
</ResponseField>

```python
import torch

# Input: (batch, sequence, embedding)
x = torch.randn(4, 128, 768)
y = mlp(x)

print(f"Input shape: {x.shape}")   # (4, 128, 768)
print(f"Output shape: {y.shape}")  # (4, 128, 768)
```

## The 4x Expansion Factor

The MLP uses a 4x expansion factor, which is standard in transformer architectures:

```python
# Example with n_embd=768
input_dim = 768
hidden_dim = 4 * 768 = 3072  # Expansion
output_dim = 768              # Projection back

# This creates a "bottleneck" architecture:
# 768 -> 3072 -> 768
```

<Tabs>
  <Tab title="Why 4x?">
    - Empirically found to work well in original transformer paper
    - Provides enough capacity for complex transformations
    - Balances model capacity with computational cost
    - Most parameters in transformer are in MLP layers
  </Tab>
  
  <Tab title="Parameter Count">
    ```python
    # For n_embd=768:
    expansion = 768 * 3072 = 2,359,296 params
    projection = 3072 * 768 = 2,359,296 params
    total = 4,718,592 params per MLP
    
    # For GPT-2 (124M) with 12 layers:
    # MLPs alone: ~56M parameters (45% of total)
    ```
  </Tab>
  
  <Tab title="Alternatives">
    Other expansion factors can be used:
    - 2x: Smaller, faster, less capacity
    - 8x: Larger, slower, more capacity
    - Variable: Different per layer
    
    But 4x is the standard choice.
  </Tab>
</Tabs>

## GELU Activation

The Gaussian Error Linear Unit (GELU) is used instead of ReLU:

```python
self.gelu = nn.GELU()
```

**GELU vs ReLU:**

| Activation | Formula | Properties |
|------------|---------|------------|
| ReLU | `max(0, x)` | Sharp cutoff at 0, faster |
| GELU | `x * Φ(x)` | Smooth, probabilistic, better for transformers |

<Note>
  GELU was introduced in the "Gaussian Error Linear Units" paper and is the standard choice for GPT models. It provides a smoother activation function that works better with transformer architectures.
</Note>

## Position-Wise Application

The MLP is applied independently to each position:

```python
# Each token is processed independently
for i in range(sequence_length):
    output[:, i, :] = mlp(input[:, i, :])

# In practice, this is done in parallel:
output = mlp(input)  # Operates on all positions at once
```

This means:
- No interaction between different positions
- Same weights used for all positions
- Highly parallelizable

## Example Usage

### Standalone Usage

```python
import torch
from model import GPTConfig, MLP

# Create MLP
config = GPTConfig(n_embd=768, dropout=0.1)
mlp = MLP(config)

# Forward pass
batch_size, seq_len, n_embd = 4, 128, 768
x = torch.randn(batch_size, seq_len, n_embd)
y = mlp(x)

print(f"Input shape: {x.shape}")
print(f"Output shape: {y.shape}")
print(f"Hidden dimension: {4 * n_embd}")
```

### Parameter Count

```python
def count_params(module):
    return sum(p.numel() for p in module.parameters())

mlp = MLP(GPTConfig(n_embd=768, bias=False))
print(f"Parameters: {count_params(mlp):,}")
# Output: 4,718,592 parameters

# Breakdown:
# c_fc: 768 * 3072 = 2,359,296
# c_proj: 3072 * 768 = 2,359,296
# Total: 4,718,592
```

### With Different Embedding Sizes

```python
# Small model
mlp_small = MLP(GPTConfig(n_embd=384))
print(count_params(mlp_small))  # ~1.2M params

# Medium model
mlp_medium = MLP(GPTConfig(n_embd=1024))
print(count_params(mlp_medium))  # ~8.4M params

# Large model
mlp_large = MLP(GPTConfig(n_embd=1600))
print(count_params(mlp_large))  # ~20.5M params
```

## Implementation Details

### Layer Naming

The layer names (`c_fc`, `c_proj`) match GPT-2's naming convention:
- `c_fc`: "Context to Fully Connected" (expansion)
- `c_proj`: "Projection" (back to embedding dimension)

This naming ensures compatibility with pretrained GPT-2 weights.

### Weight Initialization

Weights are initialized in the parent GPT class:
- `c_fc`: Normal distribution (mean=0.0, std=0.02)
- `c_proj`: Scaled by `0.02/sqrt(2*n_layer)` per GPT-2 paper

This scaled initialization prevents activation/gradient explosion in deep networks.

### Dropout Placement

Dropout is applied after the projection layer:

```python
x = self.c_fc(x)    # Expand
x = self.gelu(x)    # Activate
x = self.c_proj(x)  # Project
x = self.dropout(x) # Regularize
```

This placement:
- Regularizes the output before residual connection
- Follows standard transformer architecture
- Matches GPT-2 implementation

## Performance Characteristics

### Computational Cost

```python
# FLOPs per token
expansion_flops = n_embd * 4 * n_embd
projection_flops = 4 * n_embd * n_embd
total_flops = expansion_flops + projection_flops

# Example: n_embd=768
total_flops = 2 * 768 * 3072 = 4,718,592 FLOPs per token
```

For comparison:
- **Attention**: ~8M FLOPs per token (varies with sequence length)
- **MLP**: ~4.7M FLOPs per token (constant)

### Memory Usage

```python
# Activations per token (forward pass)
input_size = n_embd
hidden_size = 4 * n_embd  # Stored for backward pass
output_size = n_embd

# Example: n_embd=768, batch=16, seq_len=1024
total_memory = 16 * 1024 * (768 + 3072 + 768) * 4 bytes
            ≈ 303 MB per MLP layer
```

## Common Patterns

### Disable Dropout for Inference

```python
mlp.eval()  # Sets dropout to eval mode (no dropout)

# Or manually
mlp.dropout.p = 0.0
```

### Custom Expansion Factor

```python
# Not directly supported, but you can modify:
class CustomMLP(nn.Module):
    def __init__(self, config, expansion_factor=4):
        super().__init__()
        hidden_dim = expansion_factor * config.n_embd
        self.c_fc = nn.Linear(config.n_embd, hidden_dim, bias=config.bias)
        self.gelu = nn.GELU()
        self.c_proj = nn.Linear(hidden_dim, config.n_embd, bias=config.bias)
        self.dropout = nn.Dropout(config.dropout)
```

### Gradient Flow

The MLP helps gradient flow through the network:

```python
# In the transformer block:
x = x + attn(ln1(x))  # Residual connection
x = x + mlp(ln2(x))   # Residual connection

# Gradients can flow through:
# 1. The MLP path
# 2. The residual connection (identity)
```

## Comparison with Attention

| Aspect | MLP | Attention |
|--------|-----|-----------||
| Interaction | Position-wise (independent) | Across positions |
| Parameters | ~4.7M (for n_embd=768) | ~2.4M (for n_embd=768) |
| FLOPs | ~4.7M per token | ~8M per token (depends on seq_len) |
| Purpose | Non-linear transformation | Information aggregation |
| Parallelization | Trivial | More complex |

## Ablation Studies

Research has shown:
1. **Expansion factor**: 4x is optimal for most cases
2. **Activation**: GELU > ReLU for transformers
3. **Dropout**: Important for finetuning, not pretraining
4. **Position**: After attention is better than before

## See Also

- [Block](/api/block) - Transformer block containing MLP + Attention
- [CausalSelfAttention](/api/attention) - Attention mechanism
- [GPT](/api/gpt) - Main model class