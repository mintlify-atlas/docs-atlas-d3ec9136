---
title: 'GPT'
description: 'Main GPT language model class'
---

# GPT

The `GPT` class is the main language model implementation in nanoGPT. It implements a decoder-only transformer architecture based on the GPT-2 design.

## Class Definition

```python
class GPT(nn.Module):
    def __init__(self, config: GPTConfig)
```

The GPT model consists of:
- Token embeddings (`wte`)
- Position embeddings (`wpe`)
- Transformer blocks (attention + MLP)
- Final layer normalization
- Language modeling head

## Methods

### `__init__`

<ParamField path="config" type="GPTConfig" required>
  Configuration object containing model hyperparameters
</ParamField>

Initializes the GPT model with the specified configuration. The method:
- Creates token and position embeddings
- Initializes `n_layer` transformer blocks
- Sets up the language modeling head with weight tying
- Applies custom weight initialization

```python
from model import GPT, GPTConfig

config = GPTConfig(
    block_size=1024,
    vocab_size=50304,
    n_layer=12,
    n_head=12,
    n_embd=768
)
model = GPT(config)
```

### `forward`

<ParamField path="idx" type="torch.LongTensor" required>
  Input token indices of shape `(batch_size, sequence_length)`
</ParamField>

<ParamField path="targets" type="torch.LongTensor" default="None">
  Target token indices for computing loss. If provided, loss is computed and returned.
</ParamField>

<ResponseField name="logits" type="torch.FloatTensor">
  Output logits of shape `(batch_size, sequence_length, vocab_size)` during training, or `(batch_size, 1, vocab_size)` during inference.
</ResponseField>

<ResponseField name="loss" type="torch.FloatTensor or None">
  Cross-entropy loss if targets are provided, otherwise `None`.
</ResponseField>

```python
# Training forward pass
logits, loss = model(input_ids, targets=target_ids)

# Inference forward pass
logits, _ = model(input_ids)
```

<Note>
  During inference (when `targets=None`), the model only computes logits for the last position to optimize performance.
</Note>

### `generate`

<ParamField path="idx" type="torch.LongTensor" required>
  Conditioning sequence of shape `(batch_size, sequence_length)`
</ParamField>

<ParamField path="max_new_tokens" type="int" required>
  Number of new tokens to generate
</ParamField>

<ParamField path="temperature" type="float" default="1.0">
  Sampling temperature. Values < 1.0 make the model more confident, > 1.0 more random.
</ParamField>

<ParamField path="top_k" type="int" default="None">
  If set, only sample from the top k most likely tokens
</ParamField>

<ResponseField name="generated" type="torch.LongTensor">
  Generated sequence of shape `(batch_size, sequence_length + max_new_tokens)`
</ResponseField>

Autoregressively generates tokens using the model.

```python
import torch

model.eval()
with torch.no_grad():
    # Start with a prompt
    start_ids = torch.tensor([[1, 2, 3]], dtype=torch.long)
    
    # Generate 100 tokens
    generated = model.generate(
        start_ids, 
        max_new_tokens=100,
        temperature=0.8,
        top_k=200
    )
```

<Note>
  Make sure to call `model.eval()` before generation for best results.
</Note>

### `from_pretrained`

<ParamField path="model_type" type="str" required>
  One of: `'gpt2'`, `'gpt2-medium'`, `'gpt2-large'`, `'gpt2-xl'`
</ParamField>

<ParamField path="override_args" type="dict" default="None">
  Optional arguments to override. Only `'dropout'` can be overridden.
</ParamField>

<ResponseField name="model" type="GPT">
  Initialized GPT model with pretrained weights from OpenAI
</ResponseField>

Class method to load pretrained GPT-2 weights from HuggingFace.

```python
# Load pretrained GPT-2 (124M parameters)
model = GPT.from_pretrained('gpt2')

# Load GPT-2 Medium with custom dropout
model = GPT.from_pretrained('gpt2-medium', override_args={'dropout': 0.1})
```

<Tabs>
  <Tab title="Model Sizes">
    | Model Type | Parameters | Layers | Heads | Embedding Dim |
    |------------|-----------|--------|-------|---------------|
    | gpt2 | 124M | 12 | 12 | 768 |
    | gpt2-medium | 350M | 24 | 16 | 1024 |
    | gpt2-large | 774M | 36 | 20 | 1280 |
    | gpt2-xl | 1558M | 48 | 25 | 1600 |
  </Tab>
</Tabs>

### `configure_optimizers`

<ParamField path="weight_decay" type="float" required>
  Weight decay coefficient for regularization
</ParamField>

<ParamField path="learning_rate" type="float" required>
  Learning rate for the optimizer
</ParamField>

<ParamField path="betas" type="tuple" required>
  Betas for Adam optimizer, typically `(0.9, 0.95)`
</ParamField>

<ParamField path="device_type" type="str" required>
  Device type (`'cuda'` or `'cpu'`). Used to enable fused AdamW on CUDA.
</ParamField>

<ResponseField name="optimizer" type="torch.optim.AdamW">
  Configured AdamW optimizer with separate parameter groups for weight decay
</ResponseField>

Creates an AdamW optimizer with proper weight decay handling. 2D parameters (weights) get weight decay, while 1D parameters (biases, layer norms) don't.

```python
optimizer = model.configure_optimizers(
    weight_decay=0.1,
    learning_rate=6e-4,
    betas=(0.9, 0.95),
    device_type='cuda'
)
```

<Note>
  This method automatically uses fused AdamW when available on CUDA devices for better performance.
</Note>

### `estimate_mfu`

<ParamField path="fwdbwd_per_iter" type="int" required>
  Number of forward-backward passes per iteration (batch size)
</ParamField>

<ParamField path="dt" type="float" required>
  Time delta in seconds for the iteration
</ParamField>

<ResponseField name="mfu" type="float">
  Model FLOPs Utilization as a ratio (0.0 to 1.0) of A100 bfloat16 peak FLOPS
</ResponseField>

Estimates model FLOPs utilization (MFU) based on the PaLM paper calculations.

```python
import time

t0 = time.time()
# ... training iteration ...
t1 = time.time()

mfu = model.estimate_mfu(batch_size, t1 - t0)
print(f"MFU: {mfu*100:.2f}%")
```

### `get_num_params`

<ParamField path="non_embedding" type="bool" default="True">
  If True, excludes position embeddings from the count
</ParamField>

<ResponseField name="num_params" type="int">
  Total number of parameters in the model
</ResponseField>

Returns the number of parameters in the model.

```python
total_params = model.get_num_params()
print(f"Model has {total_params/1e6:.2f}M parameters")
```

### `crop_block_size`

<ParamField path="block_size" type="int" required>
  New block size (must be &lt;= current block size)
</ParamField>

Performs model surgery to decrease the context window size.

```python
# Load GPT-2 with 1024 block size
model = GPT.from_pretrained('gpt2')

# Reduce to 512 for faster training
model.crop_block_size(512)
```

## Architecture Details

The model implements:
- **Token embedding** + **position embedding** with dropout
- **Transformer blocks**: Each containing:
  - Layer normalization
  - Causal self-attention
  - Layer normalization  
  - MLP (4x expansion)
  - Residual connections
- **Final layer norm**
- **LM head** with weight tying to token embeddings

## Weight Initialization

- Linear layers: Normal distribution (mean=0.0, std=0.02)
- Embeddings: Normal distribution (mean=0.0, std=0.02)  
- Residual projections: Scaled by `0.02/sqrt(2*n_layer)` per GPT-2 paper

## See Also

- [GPTConfig](/api/gpt-config) - Configuration dataclass
- [Block](/api/block) - Transformer block implementation
- [CausalSelfAttention](/api/attention) - Attention mechanism