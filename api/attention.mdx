---
title: 'CausalSelfAttention'
description: 'Multi-head causal self-attention mechanism with Flash Attention support'
---

## Overview

`CausalSelfAttention` implements the core attention mechanism of the transformer. It performs multi-head self-attention with causal masking, ensuring each position can only attend to previous positions (autoregressive property).

Defined in `model.py:29-76`.

## Class Definition

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, config: GPTConfig)
    def forward(self, x: torch.Tensor) -> torch.Tensor
```

## Architecture

The attention module consists of:

```python
self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)  # Q, K, V projections
self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)      # Output projection
self.attn_dropout = nn.Dropout(dropout)                  # Attention dropout
self.resid_dropout = nn.Dropout(dropout)                 # Residual dropout
```

## Constructor

### `__init__(config: GPTConfig)`

Initializes the attention mechanism.

<ParamField path="config" type="GPTConfig" required>
  Configuration object. Key parameters used:
  - `n_embd`: Embedding dimension
  - `n_head`: Number of attention heads
  - `block_size`: Maximum sequence length (for causal mask)
  - `dropout`: Dropout probability
  - `bias`: Whether to use bias in linear layers
</ParamField>

**Requirements:**
- `n_embd` must be divisible by `n_head`
- Each head operates on dimension `head_dim = n_embd // n_head`

**Behavior:**
- Creates single linear layer for Q, K, V projections (3x n_embd outputs)
- Detects Flash Attention support (PyTorch >= 2.0)
- If Flash Attention unavailable, registers causal mask buffer

```python
from model import CausalSelfAttention, GPTConfig

config = GPTConfig(n_embd=768, n_head=12, dropout=0.1)
attn = CausalSelfAttention(config)

# Check attention implementation
if attn.flash:
    print("Using Flash Attention (fast)")
else:
    print("Using manual attention (slow)")
```

## Forward Pass

### `forward(x: torch.Tensor) -> torch.Tensor`

Computes multi-head causal self-attention.

<ParamField path="x" type="torch.Tensor" required>
  Input tensor of shape `(batch_size, sequence_length, n_embd)`
</ParamField>

**Returns:** `torch.Tensor` of shape `(batch_size, sequence_length, n_embd)`

**Algorithm:**

1. **Project to Q, K, V** (model.py:56)
   ```python
   q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
   ```

2. **Reshape for multi-head** (model.py:57-59)
   ```python
   B, T, C = x.size()
   k = k.view(B, T, n_head, C // n_head).transpose(1, 2)  # (B, nh, T, hs)
   q = q.view(B, T, n_head, C // n_head).transpose(1, 2)  # (B, nh, T, hs)
   v = v.view(B, T, n_head, C // n_head).transpose(1, 2)  # (B, nh, T, hs)
   ```
   where `nh` = number of heads, `hs` = head size

3. **Compute attention** - Two implementations:

   **Option A: Flash Attention** (model.py:62-64)
   ```python
   y = torch.nn.functional.scaled_dot_product_attention(
       q, k, v,
       attn_mask=None,
       dropout_p=self.dropout if self.training else 0,
       is_causal=True
   )
   ```
   
   **Option B: Manual Implementation** (model.py:66-71)
   ```python
   # Compute attention scores
   att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
   
   # Apply causal mask
   att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
   
   # Softmax and dropout
   att = F.softmax(att, dim=-1)
   att = self.attn_dropout(att)
   
   # Apply attention to values
   y = att @ v
   ```

4. **Reassemble heads** (model.py:72)
   ```python
   y = y.transpose(1, 2).contiguous().view(B, T, C)
   ```

5. **Output projection** (model.py:75)
   ```python
   y = self.resid_dropout(self.c_proj(y))
   ```

```python
import torch
from model import CausalSelfAttention, GPTConfig

config = GPTConfig(n_embd=768, n_head=12)
attn = CausalSelfAttention(config)

# Example input: batch=2, seq_len=10, embd=768
x = torch.randn(2, 10, 768)

# Forward pass
output = attn(x)
print(output.shape)  # torch.Size([2, 10, 768])
```

## Attention Mechanism Details

### Causal Masking

Causal masking ensures autoregressive generation - each position can only attend to previous positions:

```
Position:  0  1  2  3
        0 [1  0  0  0]  ← Position 0 attends only to itself
        1 [1  1  0  0]  ← Position 1 attends to 0,1
        2 [1  1  1  0]  ← Position 2 attends to 0,1,2
        3 [1  1  1  1]  ← Position 3 attends to 0,1,2,3
```

Implementation uses a lower triangular mask (model.py:49-50):
```python
self.bias = torch.tril(torch.ones(block_size, block_size))
             .view(1, 1, block_size, block_size)
```

### Multi-Head Attention

Multiple attention heads allow the model to attend to different representation subspaces:

- **Single-head**: Attends to one relationship pattern
- **Multi-head**: Each head learns different patterns (syntax, semantics, long-range dependencies, etc.)

**Example with n_head=12, n_embd=768:**
- Each head size: 768 / 12 = 64 dimensions
- 12 parallel attention computations
- Outputs concatenated and projected back to 768 dimensions

### Scaled Dot-Product Attention

The attention formula:

```
Attention(Q, K, V) = softmax(QK^T / √d_k) V
```

- **QK^T**: Computes similarity between queries and keys
- **√d_k scaling**: Prevents softmax saturation for large dimensions
- **softmax**: Converts scores to probabilities
- **Multiply by V**: Weighted sum of values

## Flash Attention vs Manual

### Flash Attention (PyTorch >= 2.0)

<ResponseField name="Advantages">
  - **Faster**: Optimized CUDA kernels
  - **Memory efficient**: O(N) memory vs O(N²) for standard attention
  - **Automatic**: Handles causal masking internally
</ResponseField>

<ResponseField name="Implementation">
  Uses `torch.nn.functional.scaled_dot_product_attention` with `is_causal=True` (model.py:64)
</ResponseField>

### Manual Implementation

<ResponseField name="When Used">
  Fallback for PyTorch < 2.0
</ResponseField>

<ResponseField name="Implementation">
  Explicit attention computation with causal mask buffer (model.py:66-71)
</ResponseField>

<ResponseField name="Drawbacks">
  - Slower (no kernel fusion)
  - More memory (stores full attention matrix)
  - Requires pre-registered causal mask buffer
</ResponseField>

## Configuration Impact

### Number of Heads

```python
# Fewer heads (6) - less diverse attention patterns
config = GPTConfig(n_embd=768, n_head=6)   # head_size = 128

# More heads (12) - more diverse patterns, standard for GPT-2
config = GPTConfig(n_embd=768, n_head=12)  # head_size = 64

# Many heads (16) - very diverse, used in GPT-2 Medium
config = GPTConfig(n_embd=1024, n_head=16) # head_size = 64
```

### Dropout

```python
# Training: use dropout for regularization
config = GPTConfig(dropout=0.1)
attn = CausalSelfAttention(config)
model.train()  # Applies 10% dropout

# Inference: disable dropout
config = GPTConfig(dropout=0.0)
attn = CausalSelfAttention(config)
model.eval()   # No dropout
```

## Usage in Transformer Block

Attention is used within a residual connection:

```python
# From Block class (model.py:104)
x = x + self.attn(self.ln_1(x))
#   ^   ^^^^^^^^^^ CausalSelfAttention
#   └─ Residual connection
```

See [Block](/api/blocks) for full context.

## Performance Considerations

<Expandable title="Memory Complexity">
  Attention has O(N²) memory complexity in sequence length:
  
  - **Attention matrix**: (batch_size, n_head, seq_len, seq_len)
  - For seq_len=1024, n_head=12: ~150M elements per batch item
  - Flash Attention reduces this significantly through kernel fusion
</Expandable>

<Expandable title="Computational Complexity">
  - **QK^T multiplication**: O(N² × d)
  - **Attention × V**: O(N² × d)
  - Total: O(N² × d) where N is sequence length, d is head dimension
  - Bottleneck for long sequences
</Expandable>

## Complete Example

```python
import torch
import torch.nn as nn
from model import CausalSelfAttention, GPTConfig

# Setup
config = GPTConfig(
    n_embd=768,
    n_head=12,
    dropout=0.1,
    block_size=1024
)
attn = CausalSelfAttention(config)

# Training
attn.train()
x = torch.randn(4, 64, 768)  # batch=4, seq=64, embd=768
output = attn(x)
print(f"Output shape: {output.shape}")  # [4, 64, 768]

# Inference
attn.eval()
with torch.no_grad():
    output = attn(x)

# Check parameters
num_params = sum(p.numel() for p in attn.parameters())
print(f"Attention parameters: {num_params:,}")  # ~2.4M for n_embd=768
```

## Related Components

- [Block](/api/blocks) - Transformer block that contains CausalSelfAttention
- [GPTConfig](/api/gpt-config) - Configuration for attention parameters
- [GPT](/api/gpt-model) - Main model using attention mechanism
