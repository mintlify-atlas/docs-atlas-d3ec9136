---
title: 'train.py'
description: 'Training script for GPT models with DDP support'
icon: 'dumbbell'
---

## Overview

The `train.py` script is a ~300-line training loop for GPT models. It supports both single-GPU training and distributed training across multiple GPUs/nodes using PyTorch's Distributed Data Parallel (DDP).

**Key Features:**
- Train from scratch, resume from checkpoint, or finetune pretrained GPT-2
- Distributed Data Parallel (DDP) for multi-GPU training
- Mixed precision training (float16/bfloat16)
- PyTorch 2.0 `torch.compile()` support
- Gradient accumulation for simulating larger batch sizes
- Cosine learning rate decay with warmup
- WandB logging integration
- Model FLOPs Utilization (MFU) tracking

**Source:** `train.py` (337 lines)

## Usage

### Single GPU Training

<CodeGroup>
```bash Basic
python train.py
```

```bash With Config File
python train.py config/train_shakespeare_char.py
```

```bash With Overrides
python train.py --batch_size=32 --compile=False
```

```bash Config + Overrides
python train.py config/train_gpt2.py --learning_rate=3e-4
```
</CodeGroup>

### Multi-GPU Training (DDP)

<CodeGroup>
```bash Single Node (4 GPUs)
torchrun --standalone --nproc_per_node=4 train.py
```

```bash Single Node (8 GPUs)
torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
```

```bash Multi-Node (2 nodes, 8 GPUs each)
# On master node (IP: 123.456.123.456):
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 \
  --master_addr=123.456.123.456 --master_port=1234 train.py

# On worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 \
  --master_addr=123.456.123.456 --master_port=1234 train.py
```

```bash Without Infiniband
# Prepend NCCL_IB_DISABLE=1 if no Infiniband
NCCL_IB_DISABLE=1 torchrun --standalone --nproc_per_node=4 train.py
```
</CodeGroup>

<Note>
  The script automatically detects DDP mode by checking the `RANK` environment variable set by `torchrun`.
</Note>

## Configuration Parameters

All configuration parameters can be overridden via command-line arguments or config files. See the [Configuration documentation](/api/configuration) for details on the override system.

### I/O Parameters

<ParamField path="out_dir" type="str" default="'out'">
  Output directory for checkpoints
</ParamField>

<ParamField path="eval_interval" type="int" default="2000">
  Number of iterations between evaluations
</ParamField>

<ParamField path="log_interval" type="int" default="1">
  Number of iterations between logging
</ParamField>

<ParamField path="eval_iters" type="int" default="200">
  Number of batches to use for evaluation
</ParamField>

<ParamField path="eval_only" type="bool" default="False">
  If True, run evaluation only and exit (useful for benchmarking)
</ParamField>

<ParamField path="always_save_checkpoint" type="bool" default="True">
  If True, always save checkpoint after evaluation. If False, only save when validation loss improves
</ParamField>

<ParamField path="init_from" type="str" default="'scratch'">
  Initialization mode:
  - `'scratch'`: Train a new model from random initialization
  - `'resume'`: Resume training from `out_dir/ckpt.pt`
  - `'gpt2'`, `'gpt2-medium'`, `'gpt2-large'`, `'gpt2-xl'`: Start from pretrained GPT-2 weights
</ParamField>

### WandB Logging

<ParamField path="wandb_log" type="bool" default="False">
  Enable Weights & Biases logging
</ParamField>

<ParamField path="wandb_project" type="str" default="'owt'">
  WandB project name
</ParamField>

<ParamField path="wandb_run_name" type="str" default="'gpt2'">
  WandB run name
</ParamField>

### Data Parameters

<ParamField path="dataset" type="str" default="'openwebtext'">
  Dataset name. Should correspond to a directory in `data/` containing `train.bin` and `val.bin`
</ParamField>

<ParamField path="gradient_accumulation_steps" type="int" default="40">
  Number of micro-batches to accumulate before updating weights. Simulates larger batch sizes.
  Effective batch size = `gradient_accumulation_steps * batch_size * num_gpus * block_size`
</ParamField>

<ParamField path="batch_size" type="int" default="12">
  Micro-batch size (per GPU if using DDP)
</ParamField>

<ParamField path="block_size" type="int" default="1024">
  Maximum sequence length (context window)
</ParamField>

### Model Parameters

<ParamField path="n_layer" type="int" default="12">
  Number of transformer layers
</ParamField>

<ParamField path="n_head" type="int" default="12">
  Number of attention heads
</ParamField>

<ParamField path="n_embd" type="int" default="768">
  Embedding dimension
</ParamField>

<ParamField path="dropout" type="float" default="0.0">
  Dropout rate. Use 0.0 for pretraining, 0.1+ for finetuning
</ParamField>

<ParamField path="bias" type="bool" default="False">
  Whether to use bias in Linear and LayerNorm layers
</ParamField>

### Optimizer Parameters

<ParamField path="learning_rate" type="float" default="6e-4">
  Maximum learning rate (after warmup, before decay)
</ParamField>

<ParamField path="max_iters" type="int" default="600000">
  Total number of training iterations
</ParamField>

<ParamField path="weight_decay" type="float" default="0.1">
  Weight decay coefficient for AdamW
</ParamField>

<ParamField path="beta1" type="float" default="0.9">
  AdamW beta1 parameter
</ParamField>

<ParamField path="beta2" type="float" default="0.95">
  AdamW beta2 parameter
</ParamField>

<ParamField path="grad_clip" type="float" default="1.0">
  Gradient clipping threshold. Set to 0.0 to disable
</ParamField>

### Learning Rate Decay

<ParamField path="decay_lr" type="bool" default="True">
  Whether to use learning rate decay
</ParamField>

<ParamField path="warmup_iters" type="int" default="2000">
  Number of warmup iterations for learning rate
</ParamField>

<ParamField path="lr_decay_iters" type="int" default="600000">
  Number of iterations for cosine decay. Usually set to `max_iters`
</ParamField>

<ParamField path="min_lr" type="float" default="6e-5">
  Minimum learning rate after decay. Usually `learning_rate / 10`
</ParamField>

### System Parameters

<ParamField path="device" type="str" default="'cuda'">
  Device to use: `'cpu'`, `'cuda'`, `'cuda:0'`, `'cuda:1'`, `'mps'` (for Apple Silicon)
</ParamField>

<ParamField path="dtype" type="str" default="'bfloat16' or 'float16'">
  Data type for training: `'float32'`, `'bfloat16'`, or `'float16'`. Default is `'bfloat16'` if supported, otherwise `'float16'`
</ParamField>

<ParamField path="compile" type="bool" default="True">
  Use PyTorch 2.0 `torch.compile()` for faster training
</ParamField>

### DDP Parameters

<ParamField path="backend" type="str" default="'nccl'">
  DDP backend: `'nccl'` (NVIDIA GPUs), `'gloo'` (CPU), etc.
</ParamField>

## Training Loop

The training loop follows this structure:

1. **Setup Phase**
   - Initialize model (scratch/resume/pretrained)
   - Setup DDP if multi-GPU
   - Configure optimizer and gradient scaler
   - Compile model if enabled

2. **Training Iteration**
   - Set learning rate (warmup/cosine decay)
   - Evaluate on train/val sets (every `eval_interval`)
   - Save checkpoint if validation improves
   - Run `gradient_accumulation_steps` micro-batches:
     - Forward pass
     - Scale loss by accumulation steps
     - Backward pass
   - Clip gradients
   - Update weights
   - Log metrics (loss, time, MFU)

3. **Termination**
   - Stop when `iter_num > max_iters`
   - Destroy DDP process group

### Learning Rate Schedule

The learning rate follows a cosine decay with linear warmup:

```python
def get_lr(it):
    # 1) Linear warmup
    if it < warmup_iters:
        return learning_rate * (it + 1) / (warmup_iters + 1)
    # 2) Return min_lr after decay period
    if it > lr_decay_iters:
        return min_lr
    # 3) Cosine decay between warmup and lr_decay_iters
    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (learning_rate - min_lr)
```

## Data Loading

The script uses a simple data loader that reads from memory-mapped binary files:

- Training data: `data/{dataset}/train.bin`
- Validation data: `data/{dataset}/val.bin`
- Metadata (optional): `data/{dataset}/meta.pkl`

**Binary Format:**
- Token IDs stored as `np.uint16` (raw bytes)
- Random sampling: batches are randomly sampled from the dataset
- Memory efficient: uses `np.memmap` to avoid loading entire dataset

## Checkpoints

Checkpoints are saved to `{out_dir}/ckpt.pt` and contain:

```python
{
    'model': model.state_dict(),
    'optimizer': optimizer.state_dict(),
    'model_args': dict(n_layer, n_head, n_embd, block_size, bias, vocab_size),
    'iter_num': current_iteration,
    'best_val_loss': best_validation_loss,
    'config': all_config_parameters
}
```

## Examples

### Train on Shakespeare (Quick Start)

```bash
# Prepare data
python data/shakespeare_char/prepare.py

# Train small model on GPU
python train.py config/train_shakespeare_char.py

# Train on CPU (slower)
python train.py config/train_shakespeare_char.py \
  --device=cpu --compile=False --eval_iters=20 \
  --block_size=64 --batch_size=12 \
  --n_layer=4 --n_head=4 --n_embd=128 \
  --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
```

### Reproduce GPT-2 (124M)

```bash
# Prepare OpenWebText dataset
python data/openwebtext/prepare.py

# Train on 8x A100 GPUs (~4 days)
torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
```

### Finetune Pretrained GPT-2

```bash
# Prepare your dataset
python data/shakespeare/prepare.py

# Finetune from GPT-2 checkpoint
python train.py config/finetune_shakespeare.py
```

### Evaluate Pretrained Models

```bash
# Evaluate GPT-2 variants on your dataset
python train.py config/eval_gpt2.py
python train.py config/eval_gpt2_medium.py
python train.py config/eval_gpt2_large.py
python train.py config/eval_gpt2_xl.py
```

## Performance Optimization

### Mixed Precision Training

The script automatically uses mixed precision (bfloat16 or float16):

- **bfloat16**: Better numerical stability, requires Ampere+ GPUs
- **float16**: Wider compatibility, uses `GradScaler` for stability

### PyTorch 2.0 Compile

Enabling `--compile=True` (default) can significantly speed up training:

- ~2x faster iteration time (e.g., 250ms â†’ 135ms)
- Requires PyTorch >= 2.0
- May have compatibility issues on some platforms (use `--compile=False`)

### Gradient Accumulation

Simulate larger batch sizes without running out of memory:

```bash
# Effective batch size = 40 * 12 * 1024 = 491,520 tokens
python train.py --gradient_accumulation_steps=40 --batch_size=12

# With 4 GPUs: 40 * 12 * 4 * 1024 = 1,966,080 tokens
torchrun --nproc_per_node=4 train.py --gradient_accumulation_steps=40
```

<Note>
  In DDP mode, `gradient_accumulation_steps` is automatically divided by the number of GPUs to maintain the same effective batch size.
</Note>

## Monitoring

### Console Output

During training, you'll see:

```
tokens per iteration will be: 491,520
number of parameters: 124.44M
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 98, with 121,344 parameters
using fused AdamW: True

step 0: train loss 10.9624, val loss 10.9520
iter 0: loss 10.9468, time 182.43ms, mfu 0.00%
iter 1: loss 10.8642, time 175.21ms, mfu 18.45%
iter 2: loss 10.2156, time 172.09ms, mfu 24.67%
...
step 2000: train loss 3.2415, val loss 3.2891
saving checkpoint to out
```

**Metrics:**
- **train/val loss**: Cross-entropy loss on training/validation sets
- **time**: Milliseconds per iteration
- **mfu**: Model FLOPs Utilization (% of A100 peak performance)

### WandB Logging

Enable with `--wandb_log=True` to track:

- Training/validation loss
- Learning rate
- MFU percentage
- Iteration number

## Troubleshooting

### Out of Memory

Reduce memory usage:

```bash
python train.py --batch_size=8 --block_size=512 --gradient_accumulation_steps=80
```

Or use a smaller model:

```bash
python train.py --n_layer=6 --n_head=6 --n_embd=384
```

### Compile Errors

Disable PyTorch 2.0 compile:

```bash
python train.py --compile=False
```

### Slow Multi-Node Training

Benchmark your network with `iperf3`. If no Infiniband:

```bash
NCCL_IB_DISABLE=1 torchrun ... train.py
```

## DDP Environment Variables

Set automatically by `torchrun`:

- `RANK`: Global rank of the process
- `LOCAL_RANK`: Local rank on the node
- `WORLD_SIZE`: Total number of processes

## See Also

- [Configuration System](/api/configuration) - How to use config files and command-line overrides
- [Model API](/api/model) - GPT model architecture details
- [Sampling](/api/sampling) - Generate text from trained models
