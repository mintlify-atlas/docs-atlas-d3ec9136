---
title: 'Block'
description: 'Transformer block combining attention and feed-forward layers'
---

# Block

The `Block` class implements a single transformer layer, combining causal self-attention and a feed-forward MLP with residual connections and layer normalization.

## Class Definition

```python
class Block(nn.Module):
    def __init__(self, config: GPTConfig)
```

## Architecture

A transformer block consists of:
1. **Layer normalization** + **Causal self-attention** + **Residual connection**
2. **Layer normalization** + **MLP** + **Residual connection**

```python
class Block(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        self.mlp = MLP(config)
    
    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
```

<Note>
  This uses **pre-normalization** (LayerNorm before attention/MLP) rather than post-normalization, which provides better gradient flow and training stability.
</Note>

## Initialization

<ParamField path="config" type="GPTConfig" required>
  Configuration object containing all model hyperparameters:
  - `n_embd`: Embedding dimension
  - `n_head`: Number of attention heads
  - `n_layer`: Number of transformer blocks (used for init scaling)
  - `dropout`: Dropout probability
  - `bias`: Whether to use bias in linear layers
  - `block_size`: Maximum sequence length
</ParamField>

```python
from model import GPTConfig, Block

config = GPTConfig(
    n_embd=768,
    n_head=12,
    dropout=0.1,
    bias=False
)
block = Block(config)
```

## Forward Pass

<ParamField path="x" type="torch.FloatTensor" required>
  Input tensor of shape `(batch_size, sequence_length, n_embd)`
</ParamField>

<ResponseField name="output" type="torch.FloatTensor">
  Output tensor of shape `(batch_size, sequence_length, n_embd)` (same as input)
</ResponseField>

```python
import torch

# Input: (batch, sequence, embedding)
x = torch.randn(4, 128, 768)
y = block(x)

print(f"Input shape: {x.shape}")   # (4, 128, 768)
print(f"Output shape: {y.shape}")  # (4, 128, 768)
```

## Information Flow

The data flows through the block as follows:

```python
# Input
x_in = x  # (B, T, C)

# First sub-layer: Attention with residual
x_norm1 = self.ln_1(x_in)          # Normalize
attn_out = self.attn(x_norm1)      # Self-attention
x = x_in + attn_out                 # Residual connection

# Second sub-layer: MLP with residual
x_norm2 = self.ln_2(x)             # Normalize
mlp_out = self.mlp(x_norm2)        # Feed-forward
x_out = x + mlp_out                 # Residual connection

return x_out
```

## Pre-Normalization vs Post-Normalization

<Tabs>
  <Tab title="Pre-Norm (nanoGPT)">
    ```python
    # Layer norm BEFORE attention/MLP
    x = x + attn(ln(x))
    x = x + mlp(ln(x))
    ```
    
    **Advantages:**
    - Better gradient flow
    - Easier to train deep networks
    - More stable training
    - No need for learning rate warmup
    
    **Used in:** GPT-2, GPT-3, most modern transformers
  </Tab>
  
  <Tab title="Post-Norm (Original)">
    ```python
    # Layer norm AFTER attention/MLP
    x = ln(x + attn(x))
    x = ln(x + mlp(x))
    ```
    
    **Advantages:**
    - Original transformer design
    - Slightly better performance (sometimes)
    
    **Disadvantages:**
    - Harder to train
    - Requires careful learning rate warmup
    - Less stable for deep networks
    
    **Used in:** Original Transformer ("Attention is All You Need")
  </Tab>
</Tabs>

## Residual Connections

Residual connections are critical for:
1. **Gradient flow**: Allow gradients to flow through identity path
2. **Training stability**: Prevent degradation in deep networks
3. **Feature preservation**: Maintain original information

```python
# Without residual (bad):
x = attn(ln(x))
x = mlp(ln(x))
# Gradients must flow through all transformations

# With residual (good):
x = x + attn(ln(x))
x = x + mlp(ln(x))
# Gradients can flow through identity path
```

<Note>
  The residual connection provides a "highway" for gradients, making it possible to train very deep networks (e.g., GPT-3 with 96 layers).
</Note>

## Example Usage

### Standalone Block

```python
import torch
from model import GPTConfig, Block

# Create a single block
config = GPTConfig(n_embd=768, n_head=12, dropout=0.1)
block = Block(config)

# Forward pass
batch_size, seq_len = 4, 128
x = torch.randn(batch_size, seq_len, config.n_embd)
y = block(x)

print(f"Input shape: {x.shape}")
print(f"Output shape: {y.shape}")
```

### Stacking Multiple Blocks

```python
import torch.nn as nn

# This is how GPT stacks blocks:
class SimplifiedGPT(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.blocks = nn.ModuleList([
            Block(config) for _ in range(config.n_layer)
        ])
    
    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x

# Example with 12 layers
config = GPTConfig(n_layer=12, n_embd=768, n_head=12)
model = SimplifiedGPT(config)

x = torch.randn(4, 128, 768)
y = model(x)
```

### Inspecting Intermediate Outputs

```python
def forward_with_intermediates(block, x):
    """Forward pass that returns intermediate values"""
    intermediates = {}
    
    # First sub-layer
    intermediates['input'] = x
    x_norm1 = block.ln_1(x)
    intermediates['ln1_out'] = x_norm1
    attn_out = block.attn(x_norm1)
    intermediates['attn_out'] = attn_out
    x = x + attn_out
    intermediates['after_attn_residual'] = x
    
    # Second sub-layer
    x_norm2 = block.ln_2(x)
    intermediates['ln2_out'] = x_norm2
    mlp_out = block.mlp(x_norm2)
    intermediates['mlp_out'] = mlp_out
    x = x + mlp_out
    intermediates['output'] = x
    
    return x, intermediates

# Use it
block = Block(GPTConfig())
x = torch.randn(2, 64, 768)
y, intermediates = forward_with_intermediates(block, x)

for key, value in intermediates.items():
    print(f"{key}: {value.shape}")
```

## Parameter Count

```python
def count_params(module):
    return sum(p.numel() for p in module.parameters())

# For n_embd=768, n_head=12
block = Block(GPTConfig(n_embd=768, n_head=12, bias=False))
print(f"Block parameters: {count_params(block):,}")
# Output: ~7.1M parameters

# Breakdown:
# - LayerNorm 1: 768 * 2 = 1,536
# - Attention: ~2.4M
#   - QKV: 768 * (3*768) = 1,769,472
#   - Output: 768 * 768 = 589,824
# - LayerNorm 2: 768 * 2 = 1,536
# - MLP: ~4.7M
#   - Expansion: 768 * 3072 = 2,359,296
#   - Projection: 3072 * 768 = 2,359,296
```

## Computational Cost

### FLOPs per Token

```python
# For n_embd=768, n_head=12, seq_len=1024

# Attention: ~8M FLOPs (depends on sequence length)
attn_flops = 2 * seq_len * n_embd * n_embd

# MLP: ~4.7M FLOPs
mlp_flops = 2 * n_embd * (4 * n_embd)

# LayerNorms: negligible

# Total per block per token: ~12-13M FLOPs
```

### Memory Usage

```python
# Activations stored for backward pass (per block)
# - Input: (B, T, C)
# - LN1 output: (B, T, C)
# - Attention output: (B, T, C)
# - After residual: (B, T, C)
# - LN2 output: (B, T, C)
# - MLP hidden: (B, T, 4*C)  # Largest activation
# - MLP output: (B, T, C)
# - Final output: (B, T, C)

# Example: B=16, T=1024, C=768
memory_per_block = 16 * 1024 * (768*7 + 3072) * 4 bytes
                ≈ 500 MB per block
```

## LayerNorm Details

The custom LayerNorm implementation supports optional bias:

```python
class LayerNorm(nn.Module):
    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None
    
    def forward(self, input):
        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)
```

<Note>
  Setting `bias=False` in LayerNorm slightly improves performance and is the default in nanoGPT.
</Note>

## Training Dynamics

### Gradient Flow

```python
# Forward:
x -> +-> LN1 -> Attn -> +-> LN2 -> MLP -> +-> output
     |                   |                |
     +-------------------+                |
     +------------------------------------+

# Backward:
# Gradients flow through:
# 1. Main path: MLP <- LN2 <- Attn <- LN1
# 2. Skip paths: Direct connections
```

The residual connections ensure gradients can flow directly, preventing vanishing gradients.

### Learning Rate Sensitivity

Pre-normalization makes the block less sensitive to learning rate:

```python
# With pre-norm, these all work reasonably well:
lr_options = [1e-4, 3e-4, 6e-4, 1e-3]

# With post-norm, need careful tuning and warmup
```

## Comparison: Attention vs MLP

| Aspect | Attention Sub-layer | MLP Sub-layer |
|--------|-------------------|---------------|
| Purpose | Aggregate information | Transform features |
| Interaction | Across positions | Position-wise |
| Parameters | ~2.4M (for n_embd=768) | ~4.7M |
| FLOPs | ~8M (seq_len=1024) | ~4.7M |
| Memory | Attention matrix: O(T²) | Hidden layer: O(4*C) |

## Common Patterns

### Freezing Layers

```python
# Freeze attention, train only MLP
block.ln_1.requires_grad_(False)
block.attn.requires_grad_(False)

# Freeze MLP, train only attention
block.ln_2.requires_grad_(False)
block.mlp.requires_grad_(False)
```

### Extracting Features

```python
# Get features from intermediate layers
def get_block_features(model, input_ids, layer_idx):
    with torch.no_grad():
        x = model.transformer.wte(input_ids)
        x = model.transformer.wpe(torch.arange(input_ids.size(1)))
        
        for i, block in enumerate(model.transformer.h):
            if i == layer_idx:
                return x
            x = block(x)
    return x
```

### Gradient Checkpointing

```python
from torch.utils.checkpoint import checkpoint

# Save memory by recomputing activations
def forward_with_checkpointing(self, x):
    x = x + checkpoint(self.attn, self.ln_1(x))
    x = x + checkpoint(self.mlp, self.ln_2(x))
    return x
```

## Design Choices

### Why Pre-Normalization?

Pre-norm provides better gradient flow:

```python
# Gradient magnitude at input:
# Pre-norm: O(1) - stable
# Post-norm: O(1/depth) - vanishes with depth
```

### Why Two Residual Connections?

Two skip connections allow gradients to bypass both attention and MLP:

```python
# Gradient can flow:
# 1. Through both attn + mlp
# 2. Skip attn, through mlp
# 3. Through attn, skip mlp
# 4. Skip both (pure residual)
```

## See Also

- [CausalSelfAttention](/api/attention) - Attention mechanism
- [MLP](/api/mlp) - Feed-forward network
- [GPT](/api/gpt) - Main model that stacks blocks
- [GPTConfig](/api/gpt-config) - Configuration for blocks