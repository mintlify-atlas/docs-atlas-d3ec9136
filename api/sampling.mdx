---
title: 'sample.py'
description: 'Generate text samples from trained or pretrained GPT models'
icon: 'wand-magic-sparkles'
---

## Overview

The `sample.py` script generates text samples from trained GPT models. It can sample from custom-trained checkpoints or pretrained GPT-2 models released by OpenAI.

**Key Features:**
- Load custom checkpoints or pretrained GPT-2 models
- Configurable sampling temperature and top-k filtering
- Support for custom prompts (text or file)
- Mixed precision inference (float16/bfloat16)
- Automatic tokenization with tiktoken or custom encoders
- Optional PyTorch 2.0 compile for faster inference

**Source:** `sample.py` (90 lines)

## Usage

### Basic Sampling

<CodeGroup>
```bash From Trained Model
# Sample from your trained model
python sample.py --out_dir=out-shakespeare-char
```

```bash From Pretrained GPT-2
# Sample from GPT-2 (124M)
python sample.py --init_from=gpt2

# Sample from GPT-2 XL (1.5B)
python sample.py --init_from=gpt2-xl
```

```bash With Custom Prompt
# Text prompt
python sample.py --init_from=gpt2 \
  --start="What is the answer to life, the universe, and everything?"

# Prompt from file
python sample.py --init_from=gpt2 --start="FILE:prompt.txt"
```

```bash Multiple Samples
# Generate 5 samples of 100 tokens each
python sample.py --init_from=gpt2 \
  --num_samples=5 --max_new_tokens=100
```
</CodeGroup>

### Advanced Sampling

<CodeGroup>
```bash Temperature and Top-K
# Lower temperature = more deterministic
python sample.py --init_from=gpt2 \
  --temperature=0.7 --top_k=100

# Higher temperature = more random
python sample.py --init_from=gpt2 \
  --temperature=1.2 --top_k=300
```

```bash CPU Inference
python sample.py --device=cpu --compile=False
```

```bash Apple Silicon (MPS)
python sample.py --device=mps
```
</CodeGroup>

## Configuration Parameters

### Model Loading

<ParamField path="init_from" type="str" default="'resume'">
  Model initialization mode:
  - `'resume'`: Load from checkpoint in `out_dir`
  - `'gpt2'`, `'gpt2-medium'`, `'gpt2-large'`, `'gpt2-xl'`: Load pretrained GPT-2 model
</ParamField>

<ParamField path="out_dir" type="str" default="'out'">
  Directory containing checkpoint (`ckpt.pt`). Only used when `init_from='resume'`
</ParamField>

### Generation Parameters

<ParamField path="start" type="str" default="'\n'">
  Prompt to start generation. Can be:
  - Plain text: `"Once upon a time"`
  - Special tokens: `"<|endoftext|>"`
  - File reference: `"FILE:prompt.txt"` (reads from file)
</ParamField>

<ParamField path="num_samples" type="int" default="10">
  Number of independent samples to generate
</ParamField>

<ParamField path="max_new_tokens" type="int" default="500">
  Number of tokens to generate per sample (excluding prompt)
</ParamField>

<ParamField path="temperature" type="float" default="0.8">
  Sampling temperature:
  - `1.0`: No change to probabilities
  - `< 1.0`: More deterministic (sharper distribution)
  - `> 1.0`: More random (flatter distribution)
  - Common range: 0.7 - 1.2
</ParamField>

<ParamField path="top_k" type="int" default="200">
  Retain only the top-k most likely tokens, set others to zero probability. Helps avoid sampling unlikely tokens that would produce nonsense.
  - Typical values: 40-200
  - Set to `None` to disable (sample from full vocabulary)
</ParamField>

<ParamField path="seed" type="int" default="1337">
  Random seed for reproducible generation
</ParamField>

### System Parameters

<ParamField path="device" type="str" default="'cuda'">
  Device for inference: `'cpu'`, `'cuda'`, `'cuda:0'`, `'mps'` (Apple Silicon)
</ParamField>

<ParamField path="dtype" type="str" default="'bfloat16' or 'float16'">
  Data type for inference: `'float32'`, `'bfloat16'`, or `'float16'`. Default is `'bfloat16'` if CUDA supports it, otherwise `'float16'`
</ParamField>

<ParamField path="compile" type="bool" default="False">
  Use PyTorch 2.0 `torch.compile()` for faster inference (requires PyTorch >= 2.0)
</ParamField>

## Encoding and Decoding

The script automatically handles tokenization:

### Custom Trained Models

If you trained on a custom dataset with `meta.pkl`:

- Loads character-level or custom tokenizer from `data/{dataset}/meta.pkl`
- Uses the `stoi` (string to index) and `itos` (index to string) mappings

**Example `meta.pkl` structure:**
```python
{
    'vocab_size': 65,
    'stoi': {'a': 0, 'b': 1, ...},  # Character to token ID
    'itos': {0: 'a', 1: 'b', ...}   # Token ID to character
}
```

### GPT-2 Models

For GPT-2 or when `meta.pkl` is not found:

- Uses OpenAI's tiktoken library with GPT-2 BPE encoding
- Supports special tokens like `<|endoftext|>`

```python
import tiktoken
enc = tiktoken.get_encoding("gpt2")
encode = lambda s: enc.encode(s, allowed_special={"<|endoftext|>"})
decode = lambda l: enc.decode(l)
```

## Examples

### Sample from Shakespeare Model

```bash
# Train model first
python train.py config/train_shakespeare_char.py

# Generate samples
python sample.py --out_dir=out-shakespeare-char
```

**Output:**
```
ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang'd
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.
---------------
DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?
---------------
...
```

### Sample from Pretrained GPT-2

```bash
python sample.py \
    --init_from=gpt2-xl \
    --start="What is the answer to life, the universe, and everything?" \
    --num_samples=5 --max_new_tokens=100
```

### Temperature Comparison

<CodeGroup>
```bash Low Temperature (0.5)
# More focused, deterministic output
python sample.py --init_from=gpt2 \
  --start="The future of AI is" \
  --temperature=0.5 --num_samples=3
```

```bash Medium Temperature (0.8)
# Balanced creativity and coherence
python sample.py --init_from=gpt2 \
  --start="The future of AI is" \
  --temperature=0.8 --num_samples=3
```

```bash High Temperature (1.5)
# More creative, diverse, but potentially less coherent
python sample.py --init_from=gpt2 \
  --start="The future of AI is" \
  --temperature=1.5 --num_samples=3
```
</CodeGroup>

### File-Based Prompts

```bash
# Create prompt file
echo "In a world where artificial intelligence" > prompt.txt

# Generate from file
python sample.py --init_from=gpt2 \
  --start="FILE:prompt.txt" \
  --max_new_tokens=200
```

### Reproducible Generation

```bash
# Same seed produces same output
python sample.py --init_from=gpt2 --seed=42 --num_samples=1
python sample.py --init_from=gpt2 --seed=42 --num_samples=1  # Identical
```

## Sampling Strategies

### Pure Sampling (No Top-K)

Sample from the full vocabulary distribution:

```bash
python sample.py --init_from=gpt2 --top_k=0
```

<Note>
  This can sometimes produce nonsensical output as it may sample very unlikely tokens.
</Note>

### Greedy Decoding (Temperature = 0.0001)

Approximate greedy decoding by using very low temperature:

```bash
python sample.py --init_from=gpt2 --temperature=0.0001
```

<Note>
  True greedy decoding would always pick the highest probability token. Very low temperature approximates this.
</Note>

### Nucleus Sampling Alternative

While the script uses top-k sampling, you can simulate nucleus sampling (top-p) with careful top-k selection:

```bash
# Conservative: top_k=40
python sample.py --init_from=gpt2 --top_k=40 --temperature=0.9

# Moderate: top_k=100
python sample.py --init_from=gpt2 --top_k=100 --temperature=0.9

# Diverse: top_k=200
python sample.py --init_from=gpt2 --top_k=200 --temperature=1.0
```

## How It Works

1. **Load Model**
   - If `init_from='resume'`: Load checkpoint from `out_dir/ckpt.pt`
   - If `init_from='gpt2*'`: Load pretrained weights from HuggingFace
   - Set model to evaluation mode

2. **Setup Tokenization**
   - Check for `meta.pkl` in dataset directory
   - Fall back to tiktoken GPT-2 encoding if not found

3. **Encode Prompt**
   - Convert start text to token IDs
   - Handle file loading if `start` begins with `FILE:`

4. **Generate Samples**
   - Call `model.generate()` for each sample
   - Decode token IDs back to text
   - Print with separator (`---------------`)

## Performance

### Faster Inference

Enable compilation for ~2x speedup:

```bash
python sample.py --init_from=gpt2 --compile=True
```

<Note>
  First run will be slower due to compilation overhead. Subsequent generations will be faster.
</Note>

### Batch Generation

The script generates samples sequentially. For true batch generation, modify the code to pass multiple prompts at once:

```python
# Current: Sequential
for k in range(num_samples):
    y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)
    print(decode(y[0].tolist()))

# Potential: Batched (requires code modification)
x_batch = x.repeat(num_samples, 1)  # Replicate prompt
y_batch = model.generate(x_batch, max_new_tokens, temperature=temperature, top_k=top_k)
for i in range(num_samples):
    print(decode(y_batch[i].tolist()))
```

## Limitations

1. **Context Length**: Generation is limited by the model's `block_size` (typically 1024). The model will automatically crop the context if it exceeds this length.

2. **Sequential Generation**: Samples are generated one at a time, not in parallel.

3. **No Beam Search**: Only supports sampling-based generation, not beam search or other structured decoding methods.

4. **Memory**: Large models (gpt2-xl) may require significant GPU memory even for inference.

## Configuration Override

Like `train.py`, this script uses the configurator system:

```bash
# Via command line
python sample.py --temperature=0.9 --top_k=100

# Via config file + overrides
python sample.py config/sample_config.py --num_samples=20
```

See [Configuration documentation](/api/configuration) for details.

## Troubleshooting

### "No meta.pkl found"

This is normal when sampling from pretrained GPT-2. The script falls back to tiktoken encoding:

```
No meta.pkl found, assuming GPT-2 encodings...
```

### Out of Memory

Reduce batch operations or use CPU:

```bash
python sample.py --device=cpu --compile=False
```

Or use a smaller model:

```bash
python sample.py --init_from=gpt2  # Instead of gpt2-xl
```

### Checkpoint Not Found

Make sure you specify the correct output directory:

```bash
python sample.py --out_dir=out-shakespeare-char
```

### Compile Errors

Disable PyTorch 2.0 compile:

```bash
python sample.py --compile=False
```

## See Also

- [Model API](/api/model) - Details on the `generate()` method
- [Training](/api/training) - Train custom models to sample from
- [Configuration](/api/configuration) - Understanding the override system
