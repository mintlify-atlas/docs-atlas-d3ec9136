---
title: 'GPTConfig'
description: 'Configuration dataclass for GPT model hyperparameters'
---

## Overview

`GPTConfig` is a dataclass that defines all hyperparameters needed to configure a GPT model. It controls the model architecture size, training behavior, and performance characteristics.

Defined in `model.py:108-116`.

## Class Definition

```python
@dataclass
class GPTConfig:
    block_size: int = 1024
    vocab_size: int = 50304
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    dropout: float = 0.0
    bias: bool = True
```

## Configuration Parameters

<ParamField path="block_size" type="int" default={1024}>
  Maximum sequence length (context window) the model can process. This determines the size of the positional embedding matrix.
</ParamField>

<ParamField path="vocab_size" type="int" default={50304}>
  Size of the vocabulary. Default is GPT-2's vocab size of 50257, padded to 50304 (nearest multiple of 64) for computational efficiency on GPUs.
</ParamField>

<ParamField path="n_layer" type="int" default={12}>
  Number of transformer blocks (layers) in the model. More layers = deeper model with more capacity but slower training.
</ParamField>

<ParamField path="n_head" type="int" default={12}>
  Number of attention heads per transformer block. Must evenly divide `n_embd`. More heads allow the model to attend to different representation subspaces simultaneously.
</ParamField>

<ParamField path="n_embd" type="int" default={768}>
  Embedding dimension size. This is the hidden size throughout the model (token embeddings, positional embeddings, attention, MLP).
  
  **Important:** Must be divisible by `n_head` since each head operates on dimension `n_embd // n_head`.
</ParamField>

<ParamField path="dropout" type="float" default={0.0}>
  Dropout probability applied throughout the model:
  - Token + position embedding dropout
  - Attention dropout (applied to attention weights)
  - Residual dropout (applied after attention and MLP projections)
  
  Set to 0.0 to disable dropout (recommended for inference).
</ParamField>

<ParamField path="bias" type="bool" default={true}>
  Whether to include bias terms in Linear layers and LayerNorm.
  
  - `True`: Include biases (GPT-2 behavior)
  - `False`: No biases (slightly faster and often performs better)
</ParamField>

## Usage Examples

### Default GPT-2 Configuration (124M parameters)

```python
from model import GPTConfig, GPT

# Use default config
config = GPTConfig()
model = GPT(config)
```

### GPT-2 Medium (350M parameters)

```python
config = GPTConfig(
    n_layer=24,
    n_head=16,
    n_embd=1024
)
model = GPT(config)
```

### Custom Small Model

```python
# Smaller, faster model for experimentation
config = GPTConfig(
    block_size=256,      # Shorter context
    vocab_size=50304,
    n_layer=6,           # Fewer layers
    n_head=6,
    n_embd=384,          # Smaller embeddings
    dropout=0.1,         # Some dropout for training
    bias=False           # No bias for efficiency
)
model = GPT(config)
```

### Training vs Inference Configuration

```python
# Training configuration
train_config = GPTConfig(
    dropout=0.1,  # Regularization during training
    bias=True
)

# Inference configuration (override dropout)
inference_config = GPTConfig(
    dropout=0.0,  # No dropout for inference
    bias=True
)
```

## Standard Model Sizes

Here are the configurations for standard GPT-2 model sizes:

| Model | n_layer | n_head | n_embd | Parameters |
|-------|---------|--------|--------|------------|
| GPT-2 | 12 | 12 | 768 | 124M |
| GPT-2 Medium | 24 | 16 | 1024 | 350M |
| GPT-2 Large | 36 | 20 | 1280 | 774M |
| GPT-2 XL | 48 | 25 | 1600 | 1558M |

<Expandable title="Why is vocab_size padded to 50304?">
  The actual GPT-2 vocabulary has 50257 tokens. Padding to 50304 (nearest multiple of 64) improves GPU computational efficiency:
  
  - GPU operations are optimized for dimensions that are multiples of powers of 2
  - Matrix multiplications benefit from aligned memory access
  - Minimal memory overhead (47 extra embedding vectors) for noticeable speed improvements
</Expandable>

## Related Classes

- [GPT](/api/gpt-model) - Main model class that uses GPTConfig
- [Block](/api/blocks) - Transformer block implementation
- [CausalSelfAttention](/api/attention) - Attention mechanism that uses n_head and n_embd
